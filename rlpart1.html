<!DOCTYPE html>
<html lang="en">
<head>
    
        <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Δ ℚuantitative √ourney | Reinforcement Learning - Part 1</title>
    <link rel="shortcut icon" type="image/png" href="http://outlace.com/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="http://outlace.com/favicon.ico">
    <link href="http://outlace.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Δ ℚuantitative √ourney Full Atom Feed" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Brandon Brown" />

    <meta name="keywords" content="RL,bandit" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="http://outlace.com/">Home</a></li>
                <li><a href="http://outlace.com/pages/about.html">About</a></li>
                <li><a href="http://outlace.com/tags/">Tags</a></li>
                <li><a href="http://outlace.com/categories/">Categories</a></li>
                <li><a href="http://outlace.com/archives/{slug}/">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="http://outlace.com/">Δ ℚuantitative √ourney</a></h1>
            <h2>Science, Math, Statistics, Machine Learning ...</h2>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Oct 19, 2015</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://outlace.com/rlpart1.html" rel="bookmark" title="Permanent Link to &quot;Reinforcement Learning - Part 1&quot;">Reinforcement Learning - Part 1</a>
                </h2>



                <div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Part-1---Action-Value-Methods-and-n-armed-bandit-problems">Part 1 - Action-Value Methods and <em>n</em>-armed bandit problems<a class="anchor-link" href="#Part-1---Action-Value-Methods-and-n-armed-bandit-problems">&#182;</a></h4>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h5 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">&#182;</a></h5><p>I'm going to begin a multipart series of posts on Reinforcement Learning (RL) that roughly follow an old 1996 textbook "Reinforcement Learning An Introduction" by Sutton and Barto. From my research, this text still seems to be the most thorough introduction to RL I could find. The Barto &amp; Sutton text is itself a great read and is fairly approachable even for beginners, but I still think it's worth breaking down even further. It still amazes me how most of machine learning theory was established decades ago yet we've seen a huge explosion of interest and use in just the past several years largely due to dramatic improvements in computational power (i.e. GPUs) and the availibility of massive data sets ("big data").  The first implementations of neural networks date back to the early 1950s!</p>
<p>While really neat results have been achieved using supervised learning models (e.g. Google's DeepDream), many consider reinforcement learning to be the holy grail of machine learning. If we can build a general learning algorithm that can learn patterns and make predictions with unlabeled data, that would be a game-changer. Google DeepMind's Deep Q-learning algorithm that learned to play dozens of old Atari games with just the raw pixel data and the score is a big step in the right direction. Clearly, there is much to be done. The algorithm still struggles with long timespan rewards (i.e. taking actions that don't result in reward for a relatively long period of time), which is why it failed to learn how to play Montezuma's Revenge and similar games. Q-learning is something that was first described in 1989, and while DeepMind's specific implementation had some novelties, it's largely the same algorithm from way back then.</p>
<p>In this series, I will be covering major topics and algorithms in RL mostly from the Barto &amp; Sutton text, but I will also include more recent advances and material where appropriate. My goal (as with all my posts) is to help those with limited mathematical backgrounds to grasp the concepts and be able to translate the equations into code (I'll use Python here). As a heads-up, the code presented here will (hopefully) maximize for readability and understandability often at the expense of computational efficiency and quality. I.e. my code will not be production-quality and is just for enhanced learning. My only assumumptions for this series is that you're proficient with Python and Numpy and have at least some basic knowledge of linear algebra and statistics/probability.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="n-armed-bandit-problem"><em>n</em>-armed bandit problem<a class="anchor-link" href="#n-armed-bandit-problem">&#182;</a></h3><p>We're going to build our way up from very simple RL algorithms to much more sophisticated ones that could be used to learn to play games, for example. The theory and math builds on each preceding part, so I strongly recommend you follow this series in order even though the first parts are less exciting.</p>
<p>Let's consider a hypothetical problem where we're at a casino and in a section with some slot machines. Let's say we're at a section with 10 slot machines in a row and it says "Play for free! Max payout is \$10!" Wow, not bad right! Let's say we ask one of the employees what's going on here, it seems too good to be true, and she says "It's really true, play as much as you want, it's free. Each slot machine is gauranteed to give you a reward between 0 and \$10. Oh, by the way, keep this on the down low but those 10 slot machines each have a different average payout, so try to figure out which one gives out the most rewards on average and you'll be making tons of cash!"</p>
<p>What kind of casino is this?! Who knows, but it's awesome. Oh by the way, here's a joke: What's another name for a slot machine? .... A one-armed bandit! Get it? It's got one arm (a lever) and it generally steals your money! Huh, well I guess we could call our situation a 10-armed bandit problem, or an <em>n</em>-armed bandit problem more generally, where <em>n</em> is the number of slot machines.</p>
<p>Let me restate our problem more formally. We have <em>n</em> possible actions (here <em>n</em> = 10) and at each play (<em>k</em>) of this "game" we can choose a single lever to pull. After taking an action $a$ we will receive a reward $R_k$ (reward at play <em>k</em>). Each lever has a unique probability distribution of payouts (rewards). For example, if we have 10 slot machines, slot machine #3 may give out an average reward of \$9 whereas slot machine \#1 only gives out an average reward of \$4. Of course, since the reward at each play is probabilistic, it is possible that lever #1 will by chance give us a reward of \$9 on a single play. But if we play many games, we expect on average slot machine #1 is associated with a lower reward than #3.</p>
<p>Thus in words, our strategy should be to play a few times, choosing different levers and observing our rewards for each action. Then we want to only choose the lever with the largest observed average reward. Thus we need a concept of <em>expected</em> reward for taking an action $a$ based on our previous plays, we'll call this expected reward $Q_k(a)$ mathematically. $Q_k(a)$ is a function that accepts action $a$ and returns the expected reward for that action. Formally,
$$Q_k(a) = \frac{R_1  +  R_2  +  {...}  +  R_k}{k_a}$$
That is, the expected reward at play <em>k</em> for action $a$ is the <em>arithmetic mean</em> of all the previous rewards we've received for taking action <em>a</em>. Thus our previous actions and observations influence our future actions, we might even say some of our previous actions <em>reinforce</em> our current and future actions. We'll come back to this later.</p>
<p>Some keywords for this problem are exploration and exploitation. Our strategy needs to include some amount of exploitation (simply choosing the best lever based on what we know so far) and some amount of exploration (choosing random levers so we can learn more). The proper balance of exploitation and exploration will be important to maximizing our rewards.</p>
<p>So how can we come up with an algorithm to figure out which slot machine has the largest average payout? Well, the simplest algorithm would be to select action $a$ for which this equation is true:
$$Q_k(A_k) = max_a(Q_k(a))$$
This equation/rule states that the expected reward for the current play <em>k</em> for taking action $A$ is equal to the maximum average reward of all previous actions taken. In other words, we use our above reward function $Q_k(a)$ on all the possible actions and select the one that returns the maximum average reward. Since $Q_k(a)$ depends on a record of our previous actions and their associated rewards, this method will not select actions that we haven't already explored. Thus we might have previously tried lever 1 and lever 3, and noticed that lever 3 gives us a higher reward, but with this method, we'll never think to try another lever, say #6, which, unbeknownst to us, actually gives out the highest average reward. This method of simply choosing the best lever that we know of so far is called a "greedy" method.</p>
<p>Obviously, we need to have some exploration of other levers (slot machines) going on to discover the true best action. One simple modification to our above algorithm is to change it to an $\epsilon$ (epsilon)-greedy algorithm, such that, with a probability $\epsilon$, we will choose an action $a$ at random, and the rest of the time (probability $1-\epsilon$) we will choose the best lever based on what we currently know from past plays. So most of the time we play greedy, but sometimes we take some risks and choose a random lever and see what happens. This will of course influence our future greedy actions.</p>
<p>Alright, I think that's an in-depth enough discussion of the problem and how we want to try to solve it with a rudimentary RL algorithm. Let's start implementing this with Python.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[579]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#imports, nothing to see here</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[595]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">arms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Per our casino example, we will be solving a 10-armed bandit problem, hence <em>n</em> = 10. I've also defined a numpy array of length <em>n</em> filled with random floats that can be understood as probabilities. The way I've chosen to implement our reward probability distributions for each arm/lever/slot machine is this: Each arm will have a probability, e.g. 0.7. The maximum reward is \$10. We will setup a for loop to 10 and at each step, it will add +1 to the reward if a random float is less than the arm's probability. Thus on the first loop, it makes up a random float (e.g. 0.4). 0.4 is less than 0.7, so reward += 1. On the next iteration, it makes up another random float (e.g. 0.6) which is also less than 0.7, thus reward += 1. This continues until we complete 10 iterations and then we return the final total reward, which could be anything 0 to 10. With an arm probability of 0.7, the <em>average</em> reward of doing this to infinity would be 7, but on any single play, it could be more or less.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[590]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">prob</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">reward</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>The next function we define is our greedy strategy of choosing the best arm so far. This function will accept a memory array that stores in a key-value sort of way the history of all actions and their rewards. It is a $2\ x\ k$ matrix where each row is an index reference to our arms array (1st element) and the reward received (2nd element). For example, if a row in our memory array is [2, 8] it means that action 2 was taken (the 3rd element in our arms array) and we received a reward of 8 for taking that action.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[596]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#initialize memory array; has 1 row defaulted to random action index</span>
<span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)),</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#av = action-value</span>

<span class="c1">#greedy method to select best arm based on memory array (historical results)</span>
<span class="k">def</span> <span class="nf">bestArm</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">bestArm</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">#just default to 0</span>
    <span class="n">bestMean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">a</span><span class="p">:</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])][:,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1">#calc mean reward for each action</span>
        <span class="k">if</span> <span class="n">bestMean</span> <span class="o">&lt;</span> <span class="n">avg</span><span class="p">:</span>
            <span class="n">bestMean</span> <span class="o">=</span> <span class="n">avg</span>
            <span class="n">bestArm</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">bestArm</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>And here is the main loop for each play. I've set it to play 500 times and display a matplotlib scatter plot of the mean reward against plays. Hopefully we'll see that the mean reward increases as we play more times.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[597]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Avg Reward&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span> <span class="c1">#greedy arm selection</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">bestArm</span><span class="p">(</span><span class="n">av</span><span class="p">)</span>
        <span class="n">thisAV</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">choice</span><span class="p">,</span> <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])]])</span>
        <span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">av</span><span class="p">,</span> <span class="n">thisAV</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1">#random arm selection</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">arms</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">arms</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">thisAV</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">choice</span><span class="p">,</span> <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])]])</span> <span class="c1">#choice, reward </span>
        <span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">av</span><span class="p">,</span> <span class="n">thisAV</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#add to our action-value memory array</span>
    <span class="c1">#calculate the percentage the correct arm is chosen (you can plot this instead of reward)</span>
    <span class="n">percCorrect</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">av</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">av</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">arms</span><span class="p">))])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">av</span><span class="p">))</span>
    <span class="c1">#calculate the mean reward</span>
    <span class="n">runningMean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">av</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">runningMean</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>




<div class="jp-RenderedImage jp-OutputArea-output ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>As you can see, the average reward does indeed improve after many plays. Our algorithm is <em>learning</em>, it is getting reinforced by previous good plays! And yet it is such a simple algorithm.</p>
<p>I encourage you to download this notebook (scroll to bottom) and experiment with different numbers of arms and different values for $\epsilon$.</p>
<p>The problem we've considered here is a <em>stationary</em> problem because the underlying reward probability distributions for each arm do not change over time. We certainly could consider a variant of this problem where this is not true, a non-stationary problem. In this case, a simple modification would be to weight more recent action-value pairs greater than distant ones, thus if things change over time, we will be able to track them. Beyond this brief mention, we will not implement this slightly more complex variant here.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Incremental-Update">Incremental Update<a class="anchor-link" href="#Incremental-Update">&#182;</a></h4><p>In our implementation we stored each action-value (action-reward) pair in a numpy array that just kept growing after each play. As you might imagine, this is not a good use of memory or computational power. Although my goal here is not to concern myself with computational efficiency, I think it's worth making our implementation more efficient in this case as it turns out to be actually simpler.</p>
<p>Instead of storing each action-value pair, we will simply keep a running tab of the <em>mean</em> reward for each action. Thus we reduce our memory array from virtually unlimited in size (as plays increase indefinitely) to a hard-limit of a 1-dimensional array of length <em>n</em> (n = # arms/levers). The index of each element corresponds to an action (e.g. 1st element corresponds to lever #1) and the value of each element is the running average of that action.</p>
<p>Then whenever we take a new action and receive a new reward, we can simply update our running average using this equation:
$$Q_{k+1} = Q_k + \frac{1}{k}[R_k - Q_k]$$
where $Q_k$ is the running average reward for action $a$ so far and $R_k$ is the reward we received right now for taking action $A_k$, and $k$ is the number of plays so far.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[720]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">arms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#initialize action-value array</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#stores counts of how many times we&#39;ve taken a particular action</span>

<span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">prob</span><span class="p">:</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">total</span>

<span class="c1">#our bestArm function is much simpler now</span>
<span class="k">def</span> <span class="nf">bestArm</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1">#returns index of element with greatest value</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">bestArm</span><span class="p">(</span><span class="n">av</span><span class="p">)</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
        <span class="n">rwd</span> <span class="o">=</span>  <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])</span>
        <span class="n">old_avg</span> <span class="o">=</span> <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
        <span class="n">new_avg</span> <span class="o">=</span> <span class="n">old_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">rwd</span> <span class="o">-</span> <span class="n">old_avg</span><span class="p">)</span> <span class="c1">#update running avg</span>
        <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_avg</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">arms</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">arms</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#randomly choose an arm (returns index)</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
        <span class="n">rwd</span> <span class="o">=</span>  <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])</span>
        <span class="n">old_avg</span> <span class="o">=</span> <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
        <span class="n">new_avg</span> <span class="o">=</span> <span class="n">old_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">rwd</span> <span class="o">-</span> <span class="n">old_avg</span><span class="p">)</span> <span class="c1">#update running avg</span>
        <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_avg</span>
    <span class="c1">#have to use np.average and supply the weights to get a weighted average</span>
    <span class="n">runningMean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">av</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">counts</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">counts</span><span class="p">))]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">runningMean</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>




<div class="jp-RenderedImage jp-OutputArea-output ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAH1JJREFUeJzt3XucXHWZ5/HP093ppEMIIUZJmCAJARVmGUlwxBngRTNL
JxGdKGZ2FV1McEaW1yKoNCyJcTEOyQAzNKgrrhOVEBgXHEDYhkFPOkoHUbnEXMBIgERAQy5cAphL
QyfpZ//4nUpXKt2dSnedOnWqv+/Xq1+pOnW66ul08nvO+T2/i7k7IiIyuNWkHYCIiKRPyUBERJQM
REREyUBERFAyEBERlAxERIQEk4GZ3WJmW83sqbxjo82szcyeNbOlZjYqqc8XEZHiJXlnsBiYXnBs
DtDm7u8BfhY/FxGRlFmSk87MbAJwv7ufHD9fB5zl7lvNbCzQ7u7vSywAEREpSrlrBke5+9b48Vbg
qDJ/voiI9CC1ArKHWxKthSEiUgHqyvx5W81srLtvMbNxwMs9nWRmShIiIv3g7taf7yv3nUErMCt+
PAu4r7cT3T2zX1/72tdSj0Hxpx/HYItd8af/NRBJDi29A/gV8F4z+6OZXQhcBzSZ2bPA38TPRUQk
ZYl1E7n7+b28dE5SnykiIv2jGcgJaGxsTDuEAVH86cly7KD4syzReQb9ZWZeiXGJiFQyM8MzUkAW
EZEKpGQgIiJKBiIiomQgIiIoGYiICEoGIiKCkoGIiKBkICIiKBmIiAhKBiIigpKBiIigZCAiIigZ
iIgISgYiIoKSgYiIoGQgIiIoGYiICEoGIlIlFi5cSEPDOzAbjllD/FWf97ivr8OprX0nxx8/mSiK
0v5RUqFtL0UkdVEUcckll/P737+Ae9dBzt4L1BYc2w3UFRzrorjr3VpgOHADAHV1zTzwwA+ZNm1a
Ed9bWQay7aWSgYjsJ4oiWloWAdDcfFGPjWJxjXdPjXZv5xXbSdFbAz8cOKzg2FvAsCLeczxwMTAr
fr6EpqZWli69p8iYKsdAkkFhKhWRCjV79mxuu+2uuPEdTkPDMObN+x/Mmzdv3zkLFy5kwYIbeeut
3ZgZw4aNYNSoel577XU6OzuBni6y8hvtvUAD0AS00db2YA/nwsEb71yjXcxFXQMwsojzoPgGXg6V
7gxE6OlKt+fGttS6G+8O+m44O4Eh8eOh5BrrcDwn11VSS2i0hxKuejf08b6FV9rDgHOAB/o4dxhw
eB/vCYfWaJfi3NM4MGZ1Ex3S91Zio6tkIEnZv/HdQ3fDmd9o9NTYFtvlUUdolC1+vBezIcAewj/p
/C6Vnvq5e5PfDdJTw1d4znZCo/5zDuw+yVfuRru3c3fRfdfRl94a+N4SZPG/t5qaYUycOJ6bb74u
k4kAlAxE9um7Lzu/8c1vVAqvdAsb20O5wtwdn1ub9329NXI99XP3plyN9gjg5Ti23s4dAfyBvhvv
Yv/OcmqB0cA29k+YPemtgS/P3VwlUzKQqld8wbKvBii/8U2icR0FbInPHQW8QXGNazGOBVbEj3tL
IrkkNhw4Kv7cs+n5LiKnsNHO3Sn1dW4tIamOpO/Gu9ir8hw15gOlZCBV5cCGv9julKH0rxBZeKVb
2NgmdVUO8EqR5w0FTgZWAW/T899HrqvkAfavG/TUfZKvsNGuo7uBL2zoc+fWYTaU4447JtPdKtVG
yUAy68CGv/DqPhRzD16whIM3xPndP/lXxIVXuoWNbbFdHnWEOkTusROKvr0VZGvzzu2toc7XfeUM
9FJ4DuecfvpJPProWnbufBOz4Rx22HCuuuoiXXFXOSUDyZQoirjwwovYvHkLodCar7D/vpSFyMKr
5Pwr4v0LiCtWrChobIvp8thLaPyHA2/mPe6k96vz6ihcSmVQMpDUFE5QAvY9P+usKSxfvnLfaytW
rODrX7+O3bvf5tCGKI7h4AXLnIMVIpPtl87/+8j9/K++uhWoY8yYd/Q6iUukFJQMJBWzZ89myZL7
CN0gbcBOwtX3aEJfeP6V+E5Cd8gwwhVzb1f7hf33XfH5xRQsc1SIlMFJyUASFUURc+dew4svbuHI
I4eza9d2Nm9+idAFkusP7wLq6e5SKZy81NNInp6GKPY0UiVXCK1n3LgxLF78LV1di/RAyUBKLlfY
3bDhObqvzOsJk5mg/yNuRgDPx497G6KofnSR/tDaRFJSofvnTrpH8owjXMEb8M68MwvH0OfG1vf0
Wm4kzzZyV/mwI/6MXUya9B41/CIp0p2B7KepqYlly54gFHb7uto/DfgJB/btdxG6igonL+XXD0KX
0qRJ71YCECmhgdwZpLK5jZnNNbO1ZvaUmf1fMxuaRhyDWRRFTJlyBu94x/FMmdJIFEVxIvgN+4/w
GUG4gs893kYoDrcBH47PNbqXYqiLn+fG2NfGX53AQzQ0jGDBgjm4v8b69auUCEQqRNnvDMxsAmH1
rBPd/W0z+xHwoLsvyTtHdwYJCt1Ad3HgRKuRhDpArrC7m+6VMrtnnnZ/z1Dq6+u5+upLNWpHpAJk
rWbwJ0IrM9zM9hI6lV9KIY5BJ4oiPv3pC9i2rRN4N6HBzy2HkEsEue6fOrqHiA4BaqipccAwe5sJ
E9THL1JNyt5N5O7bgBZCS7QJeMPdl5U7jsFm4cKFTJ/+MbZt20Po2tlBaOzfGX/lEkGu+6ce2Mno
0WP56U/vxn07e/fuYO/eN9iz52V18YhUmbLfGZjZJOBLwATCnP27zOwz7v7D/PPmz5+/73FjYyON
jY3lC7LKLFy4kK9+9QZCo/8WoRtoS/xnTi4RNAGPAXs555xTaWtrK3e4IlKk9vZ22tvbS/JeadQM
Pgk0ufs/xM8vAD7k7pfknaOaQYl0J4L8hn9b/GdPo30eA3YqEYhkUNZGE60DPmRmDWZmhGmqv0sh
jqq3fyLIjQTKJYLhhASQGwmUG+3TxYIFzUoEIoNM2buJ3H2Nmd1G2KmjC1gJLCp3HNXuwESwie6R
QH8C9jJu3Fgt7SAigCadVaUoipg+/Xy6h4luil85mlA43sGCBVdoOKhIldHaRLKf44+fzIYNr6JE
IDK4ZK1mIAmaPXs2Gza8iBKBiBwKLVRXRbrXFRqHEoGIHArdGVSJ2bNn560r9BHCrzaXCLYrEYhI
n1QzqAJh5FAL3QXjV4FZwC+BZ5k0aSzr169NM0QRKQPVDAaxKIr46lf/he7lJP5AmDPwSwBqanZz
8803phegiGSCkkGGRVHEjBkXEBaSK1xX6BlGj97Mgw/epXkEInJQ6ibKqCiKOPfcT9DVNZQwgxjg
LMJyErsYMqSTzs7tvb+BiFQddRMNMrk7gq6u0cBRwK746yVgPPAWX/vanDRDFJGM0Z1Bxux/R3AY
8EngB4TC8VvAXmbNmsGtt96aYpQikgbNQB5Ejj76vWzevIvu3cjqgM8TCsbrmDXrY0oEIoOUuokG
idmzZ7N5c25nso8QCsUjgSXAM0oEItJvSgYZEfYtbiXUCLYB3wP+nlAj2KOuIREZECWDDAiJ4G7C
3gO6IxCR0lMyqHBRFLFkyf3AkcDZ6I5ARJKgAnKFmzKlkVWrNgCjgCuAjcBi4G3Gjatj06bnU41P
RCqHCshVKooiVq9+kjC7+PfA5YQ7gv9Fbe1OFi/+bqrxiUj10J1BBZs6dSZtbb8jLDx3FrAc6KK+
vpbW1tu1zISI7EfzDKpUmFMwGjgdyHUHTaSp6XmWLr0nxchEpBINJBloc5sKtXDhQjZvfgnYAqwH
bgDA7Es0N9+ZZmgiUoV0Z1Chhg//Mzo63k24K3iUkBQamDz5CFaufCTd4ESkIunOoMpEUURHx1uE
RPBvwPXxK5dx7bX/nl5gIlK1dGdQgbqHk/6J/HWHxo0byaZNL6YbnIhULA0trSJRFLFmzW8Jq5Hu
BX5EmFvQweLFi1KNTUSql+4MKkj38tQGNKB9jEXkUOjOoEpccsmceMOai4E95PYxhre0j7GIJErJ
oEJEUcSGDX+In50M3AkcDcCIEUdogpmIJErJoEK0tCwC3kVYnvoywlDSGcDvmDPnkjRDE5FBQENL
K8Srr75GWJ46t4XlFYQtLM9j3rx5qcYmItVPBeQKcfzxf8GGDZtR0VhE+iuRSWdm9r/znjphZ5V9
z939sv58oBwoiiKef34TMJuwBtHRwOkcd5yWpxaR8uirZvCb+GsoMAV4FngOOIWw1ZaUSEvLIrq6
ZhNmG88AZlBTcwvNzRelG5iIDBoH7SYys8eAM9x9d/x8CPCIu5+WWFCDrJsozDi+EBgLLAI2MWnS
TtavfzLlyEQkS5KeZzCKsOFuzuHxMSmZPYSCcW4E0XpGjhzZ97eIiJRQMaOJrgNWmtlDhLrBWcD8
JIMafOoIhePW+PksxoxRvUBEyqfPZGBmNYRawYcIey86MMfdNw/kQ81sFPB94M/j9/ycuz86kPfM
qiiKWLt2DbCW3J4F9fVX0tx8e6pxicjgUkzNYLW7n1LSDzVbAix391vMrA44zN3fzHt90NQMeqoX
TJ5cqz0LROSQJV0zWGZmf2dm/fqAQmZ2BHCmu98C4O578hPBYBJFEatWrYmfTQPuIaxLpLmAIlJe
xbQ6FwOXA3vN7K34mLt7fyucE4FXzGwx8H7C8NUvuvuufr5fZs2dew3dxeOcK4D3phOQiAxaB70z
cPcR7l7j7kPc/fD4ayBDXeoI8xa+4+5TgJ3AnAG8X2atX/9H4D/RXTxuJRSPj0o1LhEZfIrqjzCz
I4ETgGG5Y+7+cD8/cyOw0d2fiJ/fTQ/JYP78+fseNzY20tjY2M+Pq0xRFLFjx3YKt7bUhvciUqz2
9nba29tL8l7FFJA/T1hG8xhgFWFk0a/d/W/6/aFmDwP/4O7Pmtl8oMHdr8p7veoLyFOnzqStbSJw
G/BZwnpEz7BgQbMWphORfkm6gPxF4IPAC+5+NjAZGGjB91Lgh2a2BvgL4J8G+H4ZdTJwO2E9Ipg8
+SQlAhFJRTHJ4C137wAws2Huvo4BVjjdfY27/6W7v9/dPzEYRxOdddYUCvctmDnzw+kGJSKDVjHJ
4I9xzeA+oM3MWoEXEo1qELjnnjbg83QXjj/P8uUr0w1KRAatgxaQ3f28+OF8M2snrFP00ySDqnZR
FLFmzW+BC8nNOoYl5LqLRETK7aDJwMwWAMuBX7l7e+IRDQLdS1bvq5lTU/NlmpvvSC0mERnciukm
+j3waWCFmT1hZi1m9vGE46pqYYvLkwl3A63Ad5k4cbw2vReR1BQz6ewWd78QOJswIP6/xn9Kv2nJ
ahGpLAdNBmb2AzP7FfB/CN1KM4Ejkw6sGkVRxNSpM1m//g9o1rGIVJJiZiCPjs97A9gGvJrb9UyK
F0UR5503i46O/wYsI3QRaclqEakMB52BvO9EsxOB6cCXgFp3H59YUFU4AznMOJ4BLCaMItKS1SJS
WgOZgVzMaKK/Bc6Mv0YBPwd+0Z8Pk6eA3L7G0+KvJYQEISKSnmK6iaYDDwPfcPdNCcdTtZqbL+Jn
P/sMXV1HoSWrRaTSFNVNZGYTgOPdfZmZDSd0E21PLKgq7CaC3K5mbxNWKs1NMJtIU9PzLF16T4qR
iUg1SHShOjO7CLgL+Nf40HjC0hRyiGbObAJWAz8gDCmdQX39bTQ3X5RuYCIy6BXTTXQJYdXSRwHi
ZafflWhUVSiKIv7xH28AGoDPAd8FnuHqq5s12UxEUlfMDOS33f3t3JN4A/vq68NJWEvLIjo7xwM3
EYaU/hq4KV6wTkQkXcUkg+VmNg8YbmZNhC6j+5MNq1q9ccCRF1/cmEIcIiL7KyYZzAFeIYyL/O/A
g8BXkwyqGoX9C94gjB5aEn9dwbHHjk01LhEROIRJZ/u+weyvgKvdPbGdWKptNFH37OMzCKt/nwxA
ff06WlvvVM1AREoikdFEZnammT1lZrvM7HEzO9XM/h9wM/C9/gY7GLW0LKKj43rgbiAMIR09+hUl
AhGpGH2NJvomYa/iRwkTz34JXOHu3y5HYNVrGrCFU09tVSIQkYrRVzKwvM1s7jOzF5UI+qe5+SIe
eWQWHR3heUPDVTQ3L0k3KBGRPH0lgyPM7BNArv9pSN5zd/cfJx5dlZg2bRrz5l3KjTdeA8Dll1+q
uwIRqSi9FpDN7Fb2n09g+c/jDW+SCapqC8jXA+HO4N57lyghiEhJDaSAfMijicqh2pJB9/LVs+Ij
S2hqatV6RCJSUomuTSQiItWvmLWJZIBUQBaRSqc7gzJ53/uOZ/Toa5g8ebHqBSJScYq6MzCz04EJ
eee7u9+WVFDVpLB43NFxVcoRiYgc6KAFZDP7N+A4wkL8e3PH3f3SxIKqogKyisciUi6J7oEMnAqc
VDWts4iIHKCYZPBbYByg/Y/7QcVjEcmCYrqJ2oFTgMeB3CY37u4zEguqirqJINQNWloWASE5qHgs
IklIdNKZmTX2dDxv3aKSq7ZkICJSDpqBXOF0ZyAi5ZD0ncFfAd8CTgSGArXADncf2Z8PLCqoKkoG
WpdIRMol6eUovg18GngOGAb8PfCd/nzYYNS9sc0sICSF3F2CiEilKGoGsrs/B9S6+153X0zY7EZE
RKpEMUNLd5rZUGCNmf0zsIXuPQ76zcxqgRXARnf/24G+X6XS0FIRyYJiagYTgK1APfBlYCTwHXdf
P6APNrucMKHt8MJhqtVUMwAVkEWkPBIfTWRmw4Fj3P2Z/nxID+83HrgVWAhcXnhnUG3JQESkHBIt
IJvZDGAVEMXPJ5tZa38+LM9NwJVA1wDfR0RESqCYAvJ84DTgdQB3X0VYuK5fzOyjwMvx+wy49iAi
IgNXTAF5t7u/YbZfuz2QK/q/BmaY2bmEoaojzew2d/9s/knz58/f97ixsZHGxsYBfGR6VC8QkaS0
t7fT3t5ekvcqpoB8C/AzYA7wCeAyYIi7XzzgDzc7C7iiWmsGmnAmIuWU9KSzS4E/JyxSdwfwJ+BL
/fmwXmS/1e+FJpyJSFYctJvI3XcCX4m/SsrdlwPLS/2+IiJyaHpNBmZ2P+GqvadbjkSXsK4WmnAm
IlnRa83AzF4BNhK6hh7LHY7/9PiqPpmgqqRmACogi0j5JDLpzMzqgCbgfOBk4D+AO9x9bX8DLTqo
KkoGIiLlkkgB2d33uPtP4iGfHwLWA8vN7Av9jFNERCpUnwVkMxsGfAT4FDAB+CZwb/JhiYhIOfXV
TXQ7YUjpg8CP3P2psgWlbiIRkUOWVM2gC9jZy/e5djoTEaksSdUMatz98F6+EksE1SaKIqZOncnU
qTOJoijtcEREelTUEtblVi13BlqOQkTKKfH9DMqtWpLB1KkzaWubQViOAmAJTU2tLF16T5phiUiV
SnptIhERqXLFLGEt/aTlKEQkK9RNlDAtRyEi5aKagYiIqGYgIiIDo2SQIM0xEJGsUDdRQjTHQETK
TTWDCqQ5BiJSbqoZiIjIgGieQUI0x0BEskTdRAnSHAMRKSfVDERERDUDEREZGCUDERFRMhARESUD
ERFBySARWoZCRLJGyaDEcstQtLXNoK1tIuee+xmmTGlUUhCRiqahpSXWvQzFRqAFuAnQ2kQikryB
DC3VDOQSe/XV14CngFsJiSCsTdTRAS0ti5QMRKQiqZuo5PYAi4ET0g5ERKRoujMosTFjjgLeBE4H
rtp3vKbmyzQ335FWWCIifVLNoMSiKGLGjE/R2dkFHA3sAHayYEEz8+bNSzk6EalmWo6igkybNo2r
r74CsxpgDrCA+vo6PvCBD6QdmohIr5QMErB8+Urcv0EoHs+is/Nf9q1eKiJSiZQMRESk/AVkMzsG
uA14F+DAInf/VrnjSJI2thGRrCl7AdnMxgJj3X21mY0AfgN83N2fzjsnkwXkKIqYO/caXnxxC0ce
OZyRI0czZsw7tLGNiJRFpiadufsWYEv8eIeZPU0YdvN0n99Y4bpHEdUBN7BtG9TXX0lr6+1KBCJS
8VKdZ2BmE4DJwGNpxlEKLS2L6Ox8H3AxuVnHnZ2adSwi2ZBaMoi7iO4GvujuOwpfnz9//r7HjY2N
NDY2li02EZEsaG9vp729vSTvlcqkMzMbAjwA/MTDGMzC1zNXMyjsJgJ1E4lIeQ2kZpBGAdmAJcBr
7v7lXs7JXDKA/QvIxx47nmuvnatEICJlk7VkcAbwMPAkYWgpwFx3/2neOZlMBiIiacpUMiiGkoGI
yKHT2kQiIjIgSgYiIqJkICIiSgYiIoKSgYiIoGQgIiIoGYiICEoGIiKCkoGIiKBkICIiKBmIiAhK
BiIigpKBiIigZCAiIigZlFQURUydOpOpU2cSRVHa4YiIFE37GZRIFEWcd94sOjquB6Ch4SruvXeJ
djoTkbLR5jYVYOrUmbS1zQBmxUeW0NTUytKl96QZlogMItrcRkREBqQu7QCqRXPzRTzyyCw6OsLz
hoaraG5ekm5QIiJFUjdRCUVRREvLIiAkB9ULRKScVDMQERHVDEREZGCUDEpMcw1EJIvUTVRCmmsg
ImlSzaBCaK6BiKRJNQMRERkQzTMoIc01EJGsUjdRiWmugYikRTUDERFRzUBERAZGyUBERJQMRERE
yUBERFAyEBERlAxERISUkoGZTTezdWb2nJldlUYMIiLSrezJwMxqgW8D04GTgPPN7MRyx5Gk9vb2
tEMYEMWfnizHDoo/y9K4M/ggsN7dX3D33cCdwMdSiCMxWf8HpfjTk+XYQfFnWRrJ4M+AP+Y93xgf
ExGRlKSRDLTOhIhIhSn72kRm9iFgvrtPj5/PBbrc/fq8c5QwRET6ITML1ZlZHfAM8J+BTcDjwPnu
/nRZAxERkX3Kvp+Bu+8xsy8AEVAL/ECJQEQkXRW5hLWIiJRXqjOQzey/mNlaM9trZlMKXpsbT0pb
Z2ZT846famZPxa99s/xR9y4Lk+nM7BYz22pmT+UdG21mbWb2rJktNbNRea/1+HtIi5kdY2YPxf9u
fmtml8XHM/EzmNkwM3vMzFab2e/M7Nr4eCbij+OpNbNVZnZ//DxLsb9gZk/G8T8eH8tS/KPM7G4z
ezr+93NayeJ399S+gPcB7wEeAqbkHT8JWA0MASYA6+m+i3kc+GD8+EFgepo/Q17MtXGcE+K4VwMn
ph1XD3GeCUwGnso79s/A/4wfXwVc18fvoSbl+McCp8SPRxDqTydm7GcYHv9ZBzwKnJGx+C8Hfgi0
ZvDfz/PA6IJjWYp/CfC5vH8/R5Qq/lTvDNx9nbs/28NLHwPucPfd7v4C4Yc4zczGAYe7++PxebcB
Hy9PtAeVicl07v4L4PWCwzMI/8iI/8z9nfb0e/hgOeLsjbtvcffV8eMdwNOEeSpZ+hl2xQ/rCRcR
r5OR+M1sPHAu8H0gN2olE7HnKRxtk4n4zewI4Ex3vwVC/dXd36RE8VfqQnVHEyaj5eQmphUef4nK
mbCW5cl0R7n71vjxVuCo+HFvv4eKYGYTCHc5j5Ghn8HMasxsNSHOh9x9LdmJ/ybgSqAr71hWYocw
z2mZma0ws8/Hx7IS/0TgFTNbbGYrzex7ZnYYJYo/8dFEZtZGuLUv9BV3vz/pzy+jqqjEu7sfZJ5H
RfycZjYCuAf4ortvN+u+2Kv0n8Hdu4BT4iu9yMzOLni9IuM3s48CL7v7KjNr7OmcSo09z+nuvtnM
3gm0mdm6/BcrPP46YArwBXd/wsy+AczJP2Eg8SeeDNy9qR/f9hJwTN7z8YSs9lL8OP/4S/2PrqQK
Yz6G/bNyJdtqZmPdfUvcFfdyfLyn30Pqf99mNoSQCG539/viw5n6GQDc/U0z+w/gVLIR/18DM8zs
XGAYMNLMbicbsQPg7pvjP18xs3sJ3SZZiX8jsNHdn4if3w3MBbaUIv5K6ibK78drBT5lZvVmNhE4
AXjc3bcAf4or6AZcANzXw3ulYQVwgplNMLN64JOEnyMLWoFZ8eNZdP+d9vh7SCG+feLf+w+A37n7
N/JeysTPYGZjcqM9zKwBaAJWkYH43f0r7n6Mu08EPgX83N0vIAOxA5jZcDM7PH58GDAVeIqMxB+3
f380s/fEh84B1gL3U4r4U66Mn0foZ+8AtgA/yXvtK4SCxzpgWt7xUwm/wPXAt9KMv4ef58OE0S3r
gblpx9NLjHcQZn53xn/3FwKjgWXAs8BSYNTBfg8pxn8Gob96NaERXUVYDj0TPwNwMrAyjv9J4Mr4
eCbiz4vpLLpHE2UidkKf++r467e5/6NZiT+O5/3AE8Aa4MeE0UQliV+TzkREpKK6iUREJCVKBiIi
omQgIiJKBiIigpKBiIigZCAiIigZyCBnYfn0VRaWRf/3eCIYZrYj7dhEyknJQAa7Xe4+2d1PJkzE
uzg+rgk4MqgoGYh0ewSYlH/AzEaY2TIz+028KcqM+PjXzeyLeectNLPLzGycmT2cd7dxRpl/BpF+
0QxkGdTMbLu7H25mdYTF7x5093/NO15L2Ixmu5mNAX7t7ieY2bHAj939VDOrISwF8JfA54Ch7v5P
8TpKh3nYd0GkoiW+aqlIhWsws1Xx44cJi+DlqwGuNbMzCWsiHW1m73L3F83sNTM7hbBE+0p3f93C
Voq3xCur3ufua8r1g4gMhJKBDHYd7j65j9c/A4whbMu618yeJyzfDGG3rwsJm4nkdp/6RZw4Pgrc
amY3uvvtyYUvUhqqGYj0bSRhQ5e98SY0x+a9di9hxdQPABGAmb0beMXdv09IFn0lGpGKoTsDGex6
K5rljv8QuN/MniTsWfH0vhPcd5vZz4HXvbv41ghcaWa7ge3AZxOJWqTEVEAW6ae4cPwb4O/cfUPa
8YgMhLqJRPrBzE4CngOWKRFINdCdgYiI6M5ARESUDEREBCUDERFByUBERFAyEBERlAxERAT4/0JJ
wwKSMmsgAAAAAElFTkSuQmCC
"
>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>This method achieves the same result, getting us better and better rewards over time as it learns which lever is the best option. I had to create a separate array <code>counts</code> to keep track of how many times each action is taken to properly recalculate the running reward averages for each action. Importantly, this implementation is simpler and more memory/computationally efficient.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Softmax-Action-Selection">Softmax Action Selection<a class="anchor-link" href="#Softmax-Action-Selection">&#182;</a></h3><p>Imagine another type of bandit problem: A newly minted doctor specializes in treating patients with heart attacks. She has 10 treatment options of which she can choose only one to treat each patient she sees. For some reason, all she knows is that these 10 treatments have different efficacies and risk-profiles for treating heart attacks, and she doesn't know which one is the best yet. We could still use our same $\epsilon$-greedy algorithm from above, however, we might want to reconsider our $\epsilon$ policy of completely randomly choosing a treatment once in awhile. In this new problem, randomly choosing a treatment could result in patient death, not just losing some money. So we really want to make sure to not choose the worst treatment but still have some ability to explore our options to find the best one.</p>
<p>This is where a softmax selection might be the most appropriate. Instead of just choosing an action at random during exploration, softmax gives us a probability distribution across our options. The option with the largest probability would be equivalent to our best arm action from above, but then we have some idea about what are the 2nd and 3rd best actions for example. This way, we can randomly choose to explore other options while avoiding the very worst options. Here's the softmax equation:</p>
<p><div style="font-size:20px;">
$$\frac{e^{Q_k(a)/\tau}}{\sum_{i=1}^n{e^{Q_k(i)/\tau}}}$$
</div>
$\tau$ is a parameter called temperature the scales the probability distribution of actions. A high temperature will tend the probabilities to be very simmilar, whereas a low temperature will exaggerate differences in probabilities between actions. Selecting this parameter requires an educated guess and some trial and error.</p>
<p>When we implement the slot machine 10-armed bandit problem from above using softmax, we don't need our <code>bestArm()</code> function anymore. Since softmax produces a weighted probability distribution across our possible actions, we will just randomly (but weighted) select actions according to their relative probabilities. That is, our best action will get chosen more often because it will have the highest softmax probability, but other actions will be chosen at random at lesser frequency.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[781]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">arms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#initialize action-value array, stores running reward mean</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#stores counts of how many times we&#39;ve taken a particular action</span>
<span class="c1">#stores our softmax-generated probability ranks for each action</span>
<span class="n">av_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">av_softmax</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1">#initialize each action to have equal probability</span>

<span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">prob</span><span class="p">:</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">total</span>

<span class="n">tau</span> <span class="o">=</span> <span class="mf">1.12</span> <span class="c1">#tau was selected by trial and error</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">av</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">softm</span> <span class="o">=</span> <span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">av</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">av</span><span class="p">[:]</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
        <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">softm</span>
    <span class="k">return</span> <span class="n">probs</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1">#select random arm using weighted probability distribution</span>
    <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">arms</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">arms</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">av_softmax</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
    <span class="n">rwd</span> <span class="o">=</span>  <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])</span>
    <span class="n">old_avg</span> <span class="o">=</span> <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
    <span class="n">new_avg</span> <span class="o">=</span> <span class="n">old_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">rwd</span> <span class="o">-</span> <span class="n">old_avg</span><span class="p">)</span>
    <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_avg</span>
    <span class="n">av_softmax</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">av</span><span class="p">)</span> <span class="c1">#update softmax probabilities for next play</span>
        
    <span class="n">runningMean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">av</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">counts</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">counts</span><span class="p">))]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">runningMean</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>




<div class="jp-RenderedImage jp-OutputArea-output ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Softmax action selection seems to do at least as well as epsilon-greedy, perhaps even better; it looks like it converges on an optimal policy faster. The downside to softmax is having to manually select the $\tau$ parameter. Softmax here was pretty sensitive to $\tau$ and it took awhile of playing with it to find a good value for it. Obviously with epsilon-greedy we had the parameter epsilon to set, but choosing that parameter was much more intuitive.</p>
<h3 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion">&#182;</a></h3><p>Well that concludes Part 1 of this series. While the <em>n</em>-armed bandit problem is not all that interesting, I think it does lay a good foundation for more sophisticated problems and algorithms.</p>
<p>Stay tuned for part 2 where I'll cover finite Markov decision processes and some associated algorithms.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Download-this-IPython-Notebook">Download this IPython Notebook<a class="anchor-link" href="#Download-this-IPython-Notebook">&#182;</a></h3><p><a href="https://github.com/outlace/outlace.github.io/notebooks/rlpart1.ipynb">https://github.com/outlace/outlace.github.io/notebooks/rlpart1.ipynb</a></p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="References:">References:<a class="anchor-link" href="#References:">&#182;</a></h3><ol>
<li>"Reinforcement Learning: An Introduction" Andrew Barto and Richard S. Sutton, 1996</li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network#History">https://en.wikipedia.org/wiki/Artificial_neural_network#History</a></li>
<li><a href="https://en.wikipedia.org/wiki/Q-learning">https://en.wikipedia.org/wiki/Q-learning</a></li>
</ol>

</div>
</div>



<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <div class="clear"></div>

                <div class="info">
                    <a href="http://outlace.com/rlpart1.html">posted at 00:00</a>
                    by Brandon Brown
                    &nbsp;&middot;&nbsp;<a href="http://outlace.com/category/reinforcement-learning/" rel="tag">Reinforcement-Learning</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://outlace.com/tag/rl/" class="tags">RL</a>
                    &nbsp;<a href="http://outlace.com/tag/bandit/" class="tags">bandit</a>
                </div>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'outlace';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
            </article>
            <div class="clear"></div>
            <footer>
                <p>
                <!--- <a href="http://outlace.com/feeds/all.atom.xml" rel="alternate">Atom Feed</a> --->
                <a href="mailto:outlacedev@gmail.com"><i class="svg-icon email"></i></a>
                <a href="http://github.com/outlace"><i class="svg-icon github"></i></a>
                <a href="http://outlace.com/feeds/all.atom.xml"><i class="svg-icon rss"></i></a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-65814776-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
</body>
</html>