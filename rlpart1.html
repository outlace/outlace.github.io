<!DOCTYPE html>
<html lang="en">
<head>
    
        <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Δ ℚuantitative √ourney | Reinforcement Learning - Part 1</title>
    <link rel="shortcut icon" type="image/png" href="http://outlace.com/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="http://outlace.com/favicon.ico">
    <link href="http://outlace.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Δ ℚuantitative √ourney Full Atom Feed" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Brandon Brown" />

    <meta name="keywords" content="RL,bandit" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="http://outlace.com/">Home</a></li>
                <li><a href="http://outlace.com/pages/about.html">About</a></li>
                <li><a href="http://outlace.com/tags/">Tags</a></li>
                <li><a href="http://outlace.com/categories/">Categories</a></li>
                <li><a href="http://outlace.com/archives/{slug}/">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="http://outlace.com/">Δ ℚuantitative √ourney</a></h1>
            <h2>Science, Math, Statistics, Machine Learning ...</h2>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Oct 19, 2015</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://outlace.com/rlpart1.html" rel="bookmark" title="Permanent Link to &quot;Reinforcement Learning - Part 1&quot;">Reinforcement Learning - Part 1</a>
                </h2>



                <h1>Reinforcement Learning</h1>
<h3>Part 1 - Action-Value Methods and <em>n</em>-armed bandit problems</h3>
<h5>Introduction</h5>
<p>I'm going to begin a multipart series of posts on Reinforcement Learning (RL) that roughly follow an old 1996 textbook "Reinforcement Learning An Introduction" by Sutton and Barto. From my research, this text still seems to be the most thorough introduction to RL I could find. The Barto &amp; Sutton text is itself a great read and is fairly approachable even for beginners, but I still think it's worth breaking down even further. It still amazes me how most of machine learning theory was established decades ago yet we've seen a huge explosion of interest and use in just the past several years largely due to dramatic improvements in computational power (i.e. GPUs) and the availibility of massive data sets ("big data").  The first implementations of neural networks date back to the early 1950s!</p>
<p>While really neat results have been achieved using supervised learning models (e.g. Google's DeepDream), many consider reinforcement learning to be the holy grail of machine learning. If we can build a general learning algorithm that can learn patterns and make predictions with unlabeled data, that would be a game-changer. Google DeepMind's Deep Q-learning algorithm that learned to play dozens of old Atari games with just the raw pixel data and the score is a big step in the right direction. Clearly, there is much to be done. The algorithm still struggles with long timespan rewards (i.e. taking actions that don't result in reward for a relatively long period of time), which is why it failed to learn how to play Montezuma's Revenge and similar games. Q-learning is something that was first described in 1989, and while DeepMind's specific implementation had some novelties, it's largely the same algorithm from way back then.</p>
<p>In this series, I will be covering major topics and algorithms in RL mostly from the Barto &amp; Sutton text, but I will also include more recent advances and material where appropriate. My goal (as with all my posts) is to help those with limited mathematical backgrounds to grasp the concepts and be able to translate the equations into code (I'll use Python here). As a heads-up, the code presented here will (hopefully) maximize for readability and understandability often at the expense of computational efficiency and quality. I.e. my code will not be production-quality and is just for enhanced learning. My only assumumptions for this series is that you're proficient with Python and Numpy and have at least some basic knowledge of linear algebra and statistics/probability.</p>
<h3><em>n</em>-armed bandit problem</h3>
<p>We're going to build our way up from very simple RL algorithms to much more sophisticated ones that could be used to learn to play games, for example. The theory and math builds on each preceding part, so I strongly recommend you follow this series in order even though the first parts are less exciting.</p>
<p>Let's consider a hypothetical problem where we're at a casino and in a section with some slot machines. Let's say we're at a section with 10 slot machines in a row and it says "Play for free! Max payout is \<span class="math">\(10!" Wow, not bad right! Let's say we ask one of the employees what's going on here, it seems too good to be true, and she says "It's really true, play as much as you want, it's free. Each slot machine is gauranteed to give you a reward between 0 and \\)</span>10. Oh, by the way, keep this on the down low but those 10 slot machines each have a different average payout, so try to figure out which one gives out the most rewards on average and you'll be making tons of cash!"</p>
<p>What kind of casino is this?! Who knows, but it's awesome. Oh by the way, here's a joke: What's another name for a slot machine? .... A one-armed bandit! Get it? It's got one arm (a lever) and it generally steals your money! Huh, well I guess we could call our situation a 10-armed bandit problem, or an <em>n</em>-armed bandit problem more generally, where <em>n</em> is the number of slot machines.</p>
<p>Let me restate our problem more formally. We have <em>n</em> possible actions (here <em>n</em> = 10) and at each play (<em>k</em>) of this "game" we can choose a single lever to pull. After taking an action <span class="math">\(a\)</span> we will receive a reward <span class="math">\(R_k\)</span> (reward at play <em>k</em>). Each lever has a unique probability distribution of payouts (rewards). For example, if we have 10 slot machines, slot machine #3 may give out an average reward of \<span class="math">\(9 whereas slot machine \#1 only gives out an average reward of \\)</span>4. Of course, since the reward at each play is probabilistic, it is possible that lever #1 will by chance give us a reward of \$9 on a single play. But if we play many games, we expect on average slot machine #1 is associated with a lower reward than #3.</p>
<p>Thus in words, our strategy should be to play a few times, choosing different levers and observing our rewards for each action. Then we want to only choose the lever with the largest observed average reward. Thus we need a concept of <em>expected</em> reward for taking an action <span class="math">\(a\)</span> based on our previous plays, we'll call this expected reward <span class="math">\(Q_k(a)\)</span> mathematically. <span class="math">\(Q_k(a)\)</span> is a function that accepts action <span class="math">\(a\)</span> and returns the expected reward for that action. Formally,
</p>
<div class="math">$$Q_k(a) = \frac{R_1  +  R_2  +  {...}  +  R_k}{k_a}$$</div>
<p>
That is, the expected reward at play <em>k</em> for action <span class="math">\(a\)</span> is the <em>arithmetic mean</em> of all the previous rewards we've received for taking action <em>a</em>. Thus our previous actions and observations influence our future actions, we might even say some of our previous actions <em>reinforce</em> our current and future actions. We'll come back to this later.</p>
<p>Some keywords for this problem are exploration and exploitation. Our strategy needs to include some amount of exploitation (simply choosing the best lever based on what we know so far) and some amount of exploration (choosing random levers so we can learn more). The proper balance of exploitation and exploration will be important to maximizing our rewards.</p>
<p>So how can we come up with an algorithm to figure out which slot machine has the largest average payout? Well, the simplest algorithm would be to select action <span class="math">\(a\)</span> for which this equation is true:
</p>
<div class="math">$$Q_k(A_k) = max_a(Q_k(a))$$</div>
<p>
This equation/rule states that the expected reward for the current play <em>k</em> for taking action <span class="math">\(A\)</span> is equal to the maximum average reward of all previous actions taken. In other words, we use our above reward function <span class="math">\(Q_k(a)\)</span> on all the possible actions and select the one that returns the maximum average reward. Since <span class="math">\(Q_k(a)\)</span> depends on a record of our previous actions and their associated rewards, this method will not select actions that we haven't already explored. Thus we might have previously tried lever 1 and lever 3, and noticed that lever 3 gives us a higher reward, but with this method, we'll never think to try another lever, say #6, which, unbeknownst to us, actually gives out the highest average reward. This method of simply choosing the best lever that we know of so far is called a "greedy" method.</p>
<p>Obviously, we need to have some exploration of other levers (slot machines) going on to discover the true best action. One simple modification to our above algorithm is to change it to an <span class="math">\(\epsilon\)</span> (epsilon)-greedy algorithm, such that, with a probability <span class="math">\(\epsilon\)</span>, we will choose an action <span class="math">\(a\)</span> at random, and the rest of the time (probability <span class="math">\(1-\epsilon\)</span>) we will choose the best lever based on what we currently know from past plays. So most of the time we play greedy, but sometimes we take some risks and choose a random lever and see what happens. This will of course influence our future greedy actions.</p>
<p>Alright, I think that's an in-depth enough discussion of the problem and how we want to try to solve it with a rudimentary RL algorithm. Let's start implementing this with Python.</p>
<div class="highlight"><pre><span></span><span class="c1">#imports, nothing to see here</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">arms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>


<p>Per our casino example, we will be solving a 10-armed bandit problem, hence <em>n</em> = 10. I've also defined a numpy array of length <em>n</em> filled with random floats that can be understood as probabilities. The way I've chosen to implement our reward probability distributions for each arm/lever/slot machine is this: Each arm will have a probability, e.g. 0.7. The maximum reward is \$10. We will setup a for loop to 10 and at each step, it will add +1 to the reward if a random float is less than the arm's probability. Thus on the first loop, it makes up a random float (e.g. 0.4). 0.4 is less than 0.7, so reward += 1. On the next iteration, it makes up another random float (e.g. 0.6) which is also less than 0.7, thus reward += 1. This continues until we complete 10 iterations and then we return the final total reward, which could be anything 0 to 10. With an arm probability of 0.7, the <em>average</em> reward of doing this to infinity would be 7, but on any single play, it could be more or less.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">prob</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">reward</span>
</pre></div>


<p>The next function we define is our greedy strategy of choosing the best arm so far. This function will accept a memory array that stores in a key-value sort of way the history of all actions and their rewards. It is a <span class="math">\(2\ x\ k\)</span> matrix where each row is an index reference to our arms array (1st element) and the reward received (2nd element). For example, if a row in our memory array is [2, 8] it means that action 2 was taken (the 3rd element in our arms array) and we received a reward of 8 for taking that action.</p>
<div class="highlight"><pre><span></span><span class="c1">#initialize memory array; has 1 row defaulted to random action index</span>
<span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)),</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#av = action-value</span>

<span class="c1">#greedy method to select best arm based on memory array (historical results)</span>
<span class="k">def</span> <span class="nf">bestArm</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">bestArm</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">#just default to 0</span>
    <span class="n">bestMean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">a</span><span class="p">:</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])][:,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1">#calc mean reward for each action</span>
        <span class="k">if</span> <span class="n">bestMean</span> <span class="o">&lt;</span> <span class="n">avg</span><span class="p">:</span>
            <span class="n">bestMean</span> <span class="o">=</span> <span class="n">avg</span>
            <span class="n">bestArm</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">bestArm</span>
</pre></div>


<p>And here is the main loop for each play. I've set it to play 500 times and display a matplotlib scatter plot of the mean reward against plays. Hopefully we'll see that the mean reward increases as we play more times.</p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Avg Reward&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span> <span class="c1">#greedy arm selection</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">bestArm</span><span class="p">(</span><span class="n">av</span><span class="p">)</span>
        <span class="n">thisAV</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">choice</span><span class="p">,</span> <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])]])</span>
        <span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">av</span><span class="p">,</span> <span class="n">thisAV</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1">#random arm selection</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">arms</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">arms</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">thisAV</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">choice</span><span class="p">,</span> <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])]])</span> <span class="c1">#choice, reward </span>
        <span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">av</span><span class="p">,</span> <span class="n">thisAV</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#add to our action-value memory array</span>
    <span class="c1">#calculate the percentage the correct arm is chosen (you can plot this instead of reward)</span>
    <span class="n">percCorrect</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">av</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">av</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">arms</span><span class="p">))])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">av</span><span class="p">))</span>
    <span class="c1">#calculate the mean reward</span>
    <span class="n">runningMean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">av</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">runningMean</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="rlpart1_files/rlpart1_10_0.png"></p>
<p>As you can see, the average reward does indeed improve after many plays. Our algorithm is <em>learning</em>, it is getting reinforced by previous good plays! And yet it is such a simple algorithm.</p>
<p>I encourage you to download this notebook (scroll to bottom) and experiment with different numbers of arms and different values for <span class="math">\(\epsilon\)</span>.</p>
<p>The problem we've considered here is a <em>stationary</em> problem because the underlying reward probability distributions for each arm do not change over time. We certainly could consider a variant of this problem where this is not true, a non-stationary problem. In this case, a simple modification would be to weight more recent action-value pairs greater than distant ones, thus if things change over time, we will be able to track them. Beyond this brief mention, we will not implement this slightly more complex variant here.</p>
<h4>Incremental Update</h4>
<p>In our implementation we stored each action-value (action-reward) pair in a numpy array that just kept growing after each play. As you might imagine, this is not a good use of memory or computational power. Although my goal here is not to concern myself with computational efficiency, I think it's worth making our implementation more efficient in this case as it turns out to be actually simpler.</p>
<p>Instead of storing each action-value pair, we will simply keep a running tab of the <em>mean</em> reward for each action. Thus we reduce our memory array from virtually unlimited in size (as plays increase indefinitely) to a hard-limit of a 1-dimensional array of length <em>n</em> (n = # arms/levers). The index of each element corresponds to an action (e.g. 1st element corresponds to lever #1) and the value of each element is the running average of that action.</p>
<p>Then whenever we take a new action and receive a new reward, we can simply update our running average using this equation:
</p>
<div class="math">$$Q_{k+1} = Q_k + \frac{1}{k}[R_k - Q_k]$$</div>
<p>
where <span class="math">\(Q_k\)</span> is the running average reward for action <span class="math">\(a\)</span> so far and <span class="math">\(R_k\)</span> is the reward we received right now for taking action <span class="math">\(A_k\)</span>, and <span class="math">\(k\)</span> is the number of plays so far.</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">arms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#initialize action-value array</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#stores counts of how many times we&#39;ve taken a particular action</span>

<span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">prob</span><span class="p">:</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">total</span>

<span class="c1">#our bestArm function is much simpler now</span>
<span class="k">def</span> <span class="nf">bestArm</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1">#returns index of element with greatest value</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">bestArm</span><span class="p">(</span><span class="n">av</span><span class="p">)</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
        <span class="n">rwd</span> <span class="o">=</span>  <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])</span>
        <span class="n">old_avg</span> <span class="o">=</span> <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
        <span class="n">new_avg</span> <span class="o">=</span> <span class="n">old_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">rwd</span> <span class="o">-</span> <span class="n">old_avg</span><span class="p">)</span> <span class="c1">#update running avg</span>
        <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_avg</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">arms</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">arms</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#randomly choose an arm (returns index)</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
        <span class="n">rwd</span> <span class="o">=</span>  <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])</span>
        <span class="n">old_avg</span> <span class="o">=</span> <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
        <span class="n">new_avg</span> <span class="o">=</span> <span class="n">old_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">rwd</span> <span class="o">-</span> <span class="n">old_avg</span><span class="p">)</span> <span class="c1">#update running avg</span>
        <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_avg</span>
    <span class="c1">#have to use np.average and supply the weights to get a weighted average</span>
    <span class="n">runningMean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">av</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">counts</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">counts</span><span class="p">))]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">runningMean</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="rlpart1_files/rlpart1_13_0.png"></p>
<p>This method achieves the same result, getting us better and better rewards over time as it learns which lever is the best option. I had to create a separate array <code>counts</code> to keep track of how many times each action is taken to properly recalculate the running reward averages for each action. Importantly, this implementation is simpler and more memory/computationally efficient.</p>
<h3>Softmax Action Selection</h3>
<p>Imagine another type of bandit problem: A newly minted doctor specializes in treating patients with heart attacks. She has 10 treatment options of which she can choose only one to treat each patient she sees. For some reason, all she knows is that these 10 treatments have different efficacies and risk-profiles for treating heart attacks, and she doesn't know which one is the best yet. We could still use our same <span class="math">\(\epsilon\)</span>-greedy algorithm from above, however, we might want to reconsider our <span class="math">\(\epsilon\)</span> policy of completely randomly choosing a treatment once in awhile. In this new problem, randomly choosing a treatment could result in patient death, not just losing some money. So we really want to make sure to not choose the worst treatment but still have some ability to explore our options to find the best one.</p>
<p>This is where a softmax selection might be the most appropriate. Instead of just choosing an action at random during exploration, softmax gives us a probability distribution across our options. The option with the largest probability would be equivalent to our best arm action from above, but then we have some idea about what are the 2nd and 3rd best actions for example. This way, we can randomly choose to explore other options while avoiding the very worst options. Here's the softmax equation:
<div style="font-size:20px;">
</p>
<div class="math">$$\frac{e^{Q_k(a)/\tau}}{\sum_{i=1}^n{e^{Q_k(i)/\tau}}}$$</div>
<p>
</div>
<span class="math">\(\tau\)</span> is a parameter called temperature the scales the probability distribution of actions. A high temperature will tend the probabilities to be very simmilar, whereas a low temperature will exaggerate differences in probabilities between actions. Selecting this parameter requires an educated guess and some trial and error.</p>
<p>When we implement the slot machine 10-armed bandit problem from above using softmax, we don't need our <code>bestArm()</code> function anymore. Since softmax produces a weighted probability distribution across our possible actions, we will just randomly (but weighted) select actions according to their relative probabilities. That is, our best action will get chosen more often because it will have the highest softmax probability, but other actions will be chosen at random at lesser frequency.</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">arms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">av</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#initialize action-value array, stores running reward mean</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#stores counts of how many times we&#39;ve taken a particular action</span>
<span class="c1">#stores our softmax-generated probability ranks for each action</span>
<span class="n">av_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">av_softmax</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1">#initialize each action to have equal probability</span>

<span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">prob</span><span class="p">:</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">total</span>

<span class="n">tau</span> <span class="o">=</span> <span class="mf">1.12</span> <span class="c1">#tau was selected by trial and error</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">av</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">softm</span> <span class="o">=</span> <span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">av</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">av</span><span class="p">[:]</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
        <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">softm</span>
    <span class="k">return</span> <span class="n">probs</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1">#select random arm using weighted probability distribution</span>
    <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">arms</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">arms</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">av_softmax</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
    <span class="n">rwd</span> <span class="o">=</span>  <span class="n">reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">choice</span><span class="p">])</span>
    <span class="n">old_avg</span> <span class="o">=</span> <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>
    <span class="n">new_avg</span> <span class="o">=</span> <span class="n">old_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">rwd</span> <span class="o">-</span> <span class="n">old_avg</span><span class="p">)</span>
    <span class="n">av</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_avg</span>
    <span class="n">av_softmax</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">av</span><span class="p">)</span> <span class="c1">#update softmax probabilities for next play</span>

    <span class="n">runningMean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">av</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">counts</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">counts</span><span class="p">))]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">runningMean</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="rlpart1_files/rlpart1_16_0.png"></p>
<p>Softmax action selection seems to do at least as well as epsilon-greedy, perhaps even better; it looks like it converges on an optimal policy faster. The downside to softmax is having to manually select the <span class="math">\(\tau\)</span> parameter. Softmax here was pretty sensitive to <span class="math">\(\tau\)</span> and it took awhile of playing with it to find a good value for it. Obviously with epsilon-greedy we had the parameter epsilon to set, but choosing that parameter was much more intuitive.</p>
<h3>Conclusion</h3>
<p>Well that concludes Part 1 of this series. While the <em>n</em>-armed bandit problem is not all that interesting, I think it does lay a good foundation for more sophisticated problems and algorithms.</p>
<p>Stay tuned for part 2 where I'll cover finite Markov decision processes and some associated algorithms.</p>
<h3>Download this IPython Notebook</h3>
<p>https://github.com/outlace/outlace.github.io/notebooks/rlpart1.ipynb</p>
<h3>References:</h3>
<ol>
<li>"Reinforcement Learning: An Introduction" Andrew Barto and Richard S. Sutton, 1996</li>
<li>https://en.wikipedia.org/wiki/Artificial_neural_network#History</li>
<li>https://en.wikipedia.org/wiki/Q-learning</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <div class="clear"></div>

                <div class="info">
                    <a href="http://outlace.com/rlpart1.html">posted at 00:00</a>
                    by Brandon Brown
                    &nbsp;&middot;&nbsp;<a href="http://outlace.com/category/reinforcement-learning/" rel="tag">Reinforcement-Learning</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://outlace.com/tag/rl/" class="tags">RL</a>
                    &nbsp;<a href="http://outlace.com/tag/bandit/" class="tags">bandit</a>
                </div>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'outlace';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
            </article>
            <div class="clear"></div>
            <footer>
                <p>
                <!--- <a href="http://outlace.com/feeds/all.atom.xml" rel="alternate">Atom Feed</a> --->
                <a href="mailto:outlacedev@gmail.com"><i class="svg-icon email"></i></a>
                <a href="http://github.com/outlace"><i class="svg-icon github"></i></a>
                <a href="http://outlace.com/feeds/all.atom.xml"><i class="svg-icon rss"></i></a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-65814776-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
</body>
</html>