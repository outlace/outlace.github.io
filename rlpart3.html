<!DOCTYPE html>
<html lang="en">
<head>
    
        <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Δ ℚuantitative √ourney | Q-learning with Neural Networks</title>
    <link rel="shortcut icon" type="image/png" href="http://outlace.com/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="http://outlace.com/favicon.ico">
    <link href="http://outlace.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Δ ℚuantitative √ourney Full Atom Feed" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Brandon Brown" />

    <meta name="keywords" content="Q-learning,RL" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="http://outlace.com/">Home</a></li>
                <li><a href="http://outlace.com/pages/about.html">About</a></li>
                <li><a href="http://outlace.com/tags/">Tags</a></li>
                <li><a href="http://outlace.com/categories/">Categories</a></li>
                <li><a href="http://outlace.com/archives/{slug}/">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="http://outlace.com/">Δ ℚuantitative √ourney</a></h1>
            <h2>Science, Math, Statistics, Machine Learning ...</h2>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Oct 30, 2015</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://outlace.com/rlpart3.html" rel="bookmark" title="Permanent Link to &quot;Q-learning with Neural Networks&quot;">Q-learning with Neural Networks</a>
                </h2>



                <h3>Learning Gridworld with Q-learning</h3>
<h4>Introduction</h4>
<p>We've finally made it. We've made it to what we've all been waiting for, Q-learning with neural networks. Since I'm sure a lot of people didn't follow parts 1 and 2 because they were kind of boring, I will attempt to make this post relatively (but not completely) self-contained. In this post, we will dive into using Q-learning to train an agent (player) how to play Gridworld. Gridworld is a simple text based game in which there is a 4x4 grid of tiles and 4 objects placed therein: a player, pit, goal, and a wall. The player can move up/down/left/right (<span class="math">\(a \in A \{up,down,left,right\}\)</span>) and the point of the game is to get to the goal where the player will receive a numerical reward. Unfortunately, we have to avoid a pit, because if we land on the pit we are penalized with a negative 'reward'. As if our task wasn't difficult enough, there's also a wall that can block the player's path (but it offers no reward or penalty).</p>
<p><img src="images/RL/gridworld.png" /></p>
<h4>Quick Review of Terms and Concepts (skip if you followed parts 1 &amp; 2)</h4>
<p>A state is all the information necessary (e.g. pixel data in a game) to make a decision that you expect will take you to a new (higher value) state. The high level function of reinforcement learning is to learn the values of states or state-action pairs (the value of taking action <span class="math">\(a\)</span> given we're in state <span class="math">\(s\)</span>). The value is some notion of how "good" that state or action is. Generally this is a function of rewards received now or in the future as a result of taking some action or being in some state.</p>
<p>A policy, denoted <span class="math">\(\pi\)</span>, is the specific strategy we take in order to get into high value states or take high value actions to maximize our rewards over time. For example, a policy in blackjack might be to always hit until we have 19. We denote a function, <span class="math">\(\pi(s)\)</span> that accepts a state <span class="math">\(s\)</span> and returns the action to be taken. Generally <span class="math">\(\pi(s)\)</span> as a function just evaluates the value of all possible actions given the state <span class="math">\(s\)</span> and returns the highest value action. This will result in a specific policy <span class="math">\(\pi\)</span> that may change over time as we improve our value estimates.</p>
<p>We call the function that accepts a state <span class="math">\(s\)</span> and returns the value of that state <span class="math">\(v_{\pi}(s)\)</span>. This is the value function. Similarly, there is an action-value function <span class="math">\(Q(s, a)\)</span> that accepts a state <span class="math">\(s\)</span> and an action <span class="math">\(a\)</span> and returns the value of taking that action given that state. Some RL algorithms or implementations will use one or the other. Importantly, if we base our algorithm on learning state-values (as opposed to action-values), we must keep in mind that the value of a state depends completely on our policy <span class="math">\(\pi\)</span>. Using blackjack as an example, if we're in the state of having a card total of 20, and have two possible actions, hit or stay, the value of this state is only high if our policy says to stay when we have 20. If our policy said to hit when we have 20, we would probably bust and lose the game, thus the value of that state would be low. More formally, the value of a state is equivalent to the value of the highest action taken in that state.</p>
<h4>What is Q-learning?</h4>
<p>Q-learning, like virtually all RL methods, is one type of algorithm used to calculate state-action values. It falls under the class of <em>temporal difference</em> (TD) algorithms, which suggests that time differences between actions taken and rewards received are involved.</p>
<p>In part 2 where we used a Monte Carlo method to learn to play blackjack, we had to wait until the end of a game (episode) to update our state-action values. With TD algorithms, we make updates after every action taken. In most cases, that makes more sense. We make a prediction (based on previous experience), take an action based on that prediction, receive a reward and then update our prediction.</p>
<p>(Btw: Don't confuse the "Q" in Q-learning with the <span class="math">\(Q\)</span> function we've discussed in the previous parts. The <span class="math">\(Q\)</span> function is always the name of the function that accepts states and actions and spits out the value of that state-action pair. RL methods involve a <span class="math">\(Q\)</span> function but aren't necessarily Q-learning algorithms.)</p>
<p>Here's the tabular Q-learning update rule:
</p>
<div class="math">$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma maxQ(S_{t+1}, a_{t+1}) - Q(S_t, A_t)]$$</div>
<p>So, like Monte Carlo, we could have a table that stores the Q-value for every possible state-action pair and iteratively update this table as we play games. Our policy <span class="math">\(\pi\)</span> would be based on choosing the action with the highest Q value for that given state.</p>
<p>But we're done with tables. This is 2015, we have GPUs and stuff. Well, as I alluded to in part 2, our <span class="math">\(Q(s,a)\)</span> function doesn't have to just be a lookup table. In fact, in most interesting problems, our state-action space is much too large to store in a table. Imagine a very simplified game of Pacman. If we implement it as a graphics-based game, the state would be the raw pixel data. In a tabular method, if the pixel data changes by just a single pixel, we have to store that as a completely separate entry in the table. Obviously that's silly and wasteful. What we need is some way to generalize and pattern match between states. We need our algorithm to say "the value of these <em>kind</em> of states is X" rather than "the value of this exact, super specific state is X."</p>
<p>That's where neural networks come in. Or any other type of function approximator, even a simple linear model. We can use a neural network, instead of a lookup table, as our <span class="math">\(Q(s,a)\)</span> function. Just like before, it will accept a state and an action and spit out the value of that state-action.</p>
<p>Importantly, however, unlike a lookup table, a neural network also has a bunch of parameters associated with it. These are the weights. So our <span class="math">\(Q\)</span> function actually looks like this: <span class="math">\(Q(s, a, \theta)\)</span> where <span class="math">\(\theta\)</span> is a vector of parameters. And instead of iteratively updating values in a table, we will iteratively update the <span class="math">\(\theta\)</span> parameters of our neural network so that it learns to provide us with better estimates of state-action values.</p>
<p>Of course we can use gradient descent (backpropagation) to train our <span class="math">\(Q\)</span> neural network just like any other neural network.</p>
<p>But what's our target <code>y</code> vector (expected output vector)? Since the net is not a table, we don't use the formula shown above, our target is simply: <span class="math">\(r_{t+1} + \gamma * maxQ(s', a')\)</span> for the state-action that just happened. <span class="math">\(\gamma\)</span> is a parameter <span class="math">\(0\rightarrow1\)</span> that is called the <em>discount factor</em>. Basically it determines how much each future reward is taken into consideration for updating our Q-value. If <span class="math">\(\gamma\)</span> is close to 0, we heavily discount future rewards and thus mostly care about immediate rewards.  <span class="math">\(s'\)</span> refers to the new state after having taken action <span class="math">\(a\)</span> and <span class="math">\(a'\)</span> refers to the next actions possible in this new state. So <span class="math">\(maxQ(s', a')\)</span> means we calculate all the Q-values for each state-action pair in the new state, and take the maximium value to use in our new value update. (Note I may use <span class="math">\(s' \text{ and } a'\)</span> interchangeably with <span class="math">\(s_{t+1} \text{ and } a_{t+1}\)</span>.)</p>
<p>One important note: our reward update for every state-action pair is <span class="math">\(r_{t+1} + \gamma*maxQ(s_{t+1}, a)\)</span> <strong>except</strong> when the state <span class="math">\(s'\)</span> is a terminal state. When we've reached a terminal state, the reward update is simply <span class="math">\(r_{t+1}\)</span>. A terminal state is the last state in an episode. In our case, there are 2 terminal states: the state where the player fell into the pit (and receives -10) and the state where the player has reached the goal (and receives +10). Any other state is non-terminal and the game is still in progress.</p>
<p>There are two keywords I need to mention as well: <strong>on-policy</strong> and <strong>off-policy</strong> methods. In on-policy methods we iteratively learn about state values at the same time that we improve our policy. In other words, the updates to our state values depend on the policy. In contrast, off-policy methods do not depend on the policy to update the value function. Q-learning is an off-policy method. It's advantageous because with off-policy methods, we can follow one policy while learning about another. For example, with Q-learning, we could always take completely random actions and yet we would still learn about another policy function of taking the best actions in every state. If there's ever a <span class="math">\(\pi\)</span> referenced in the value update part of the algorithm then it's an on-policy method.</p>
<h3>Gridworld Details</h3>
<p>Before we get too deep into the neural network Q-learning stuff, let's discuss the Gridworld game implementation that we're using as our toy problem.</p>
<p>We're going to implement 3 variants of the game in order of increasing difficulty. The first version will initialize a grid in exactly the same way each time. That is, every new game starts with the player (P), goal (+), pit (-), and wall (W) in exactly the same positions. Thus the algorithm just needs to learn how to take the player from a known starting position to a known end position without hitting the pit, which gives out negative rewards.</p>
<p>The second implementation is slightly more difficult. The goal, pit and wall will always be initialized in the same positions, but the player will be placed randomly on the grid on each new game. The third implementation is the most difficult to learn, and that's where all elements are randomly placed on the grid each game.</p>
<p>Let's get to coding.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">randPair</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">)</span>

<span class="c1">#finds an array in the &quot;depth&quot; dimension of the grid</span>
<span class="k">def</span> <span class="nf">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">obj</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span>

<span class="c1">#Initialize stationary grid, all items are placed deterministically</span>
<span class="k">def</span> <span class="nf">initGrid</span><span class="p">():</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="c1">#place player</span>
    <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1">#place wall</span>
    <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#place pit</span>
    <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#place goal</span>
    <span class="n">state</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">state</span>

<span class="c1">#Initialize player in random location, but keep wall, goal and pit stationary</span>
<span class="k">def</span> <span class="nf">initGridPlayer</span><span class="p">():</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="c1">#place player</span>
    <span class="n">state</span><span class="p">[</span><span class="n">randPair</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1">#place wall</span>
    <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#place pit</span>
    <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#place goal</span>
    <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span> <span class="c1">#find grid position of player (agent)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span> <span class="c1">#find wall</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span> <span class="c1">#find goal</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span> <span class="c1">#find pit</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">a</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">w</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">g</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="p">):</span>
        <span class="c1">#print(&#39;Invalid grid. Rebuilding..&#39;)</span>
        <span class="k">return</span> <span class="n">initGridPlayer</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">state</span>

<span class="c1">#Initialize grid so that goal, pit, wall, player are all randomly placed</span>
<span class="k">def</span> <span class="nf">initGridRand</span><span class="p">():</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="c1">#place player</span>
    <span class="n">state</span><span class="p">[</span><span class="n">randPair</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1">#place wall</span>
    <span class="n">state</span><span class="p">[</span><span class="n">randPair</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#place pit</span>
    <span class="n">state</span><span class="p">[</span><span class="n">randPair</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#place goal</span>
    <span class="n">state</span><span class="p">[</span><span class="n">randPair</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="c1">#If any of the &quot;objects&quot; are superimposed, just call the function again to re-place</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">a</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">w</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">g</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="p">):</span>
        <span class="c1">#print(&#39;Invalid grid. Rebuilding..&#39;)</span>
        <span class="k">return</span> <span class="n">initGridRand</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">state</span>
</code></pre></div>

<p>The state is a 3-dimensional numpy array (4x4x4). You can think of the first two dimensions as the positions on the board; e.g. row 1, column 2 is the position (1,2) [zero indexed] on the board. The 3rd dimension encodes the object/element at that position. Since there are 4 different possible objects, the 3rd dimension of the state contains vectors of length 4. We're using a one-hot encoding for the elements except that the empty position is just a vector of all zeros. So with a 4 length vector we're encoding 5 possible options at each grid position: empty, player, goal, pit, or wall.</p>
<p>You can also think of the 3rd dimension as being divided into 4 separate grid planes, where each plane represents the position of each element. So below is an example where the player is at grid position (3,0), the wall is at (0,0), the pit is at (0,1) and the goal is at (1,0). [All other elements are 0s]</p>
<p><img src="images/RL/gridpositions.png" width="300px" /></p>
<p>In our simple implementation it's possible for the board to be initialized such that some of the objects contain a 1 at the same "x,y" position (but different "z" positions), which indicates they're at the same position on the grid. Obviously we don't want to initialize the board in this way, so for the last 2 variants of the game that involve some element of random initialization, we check if we can find "clean" arrays (only one "1" in the 'Z' dimension of a particular grid position) of the various element types on the grid and if not, we just recursively call the initialize grid function until we get a state where elements are not superimposed.</p>
<p>When the player successfully plays the game and lands on the goal, the player and goal positions <em>will</em> be superimposed and that is how we know the player has won (likewise if the player hits the pit and loses). The wall is supposed to block the movement of the player so we prevent the player from taking an action that would place them at the same position as the wall. Additionally, the grid is "enclosed" so that player cannot walk through the edges of the grid.</p>
<p>Now we will implement the movement function.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">makeMove</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="c1">#need to locate player in grid</span>
    <span class="c1">#need to determine what object (if any) is in the new grid spot the player is moving to</span>
    <span class="n">player_loc</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">wall</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">goal</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">pit</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="n">actions</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
    <span class="c1">#e.g. up =&gt; (player row - 1, player column + 0)</span>
    <span class="n">new_loc</span> <span class="o">=</span> <span class="p">(</span><span class="n">player_loc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">actions</span><span class="p">[</span><span class="n">action</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">player_loc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">actions</span><span class="p">[</span><span class="n">action</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">new_loc</span> <span class="o">!=</span> <span class="n">wall</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_loc</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_loc</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">()):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">new_loc</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">new_player_loc</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">new_player_loc</span><span class="p">):</span>
        <span class="n">state</span><span class="p">[</span><span class="n">player_loc</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1">#re-place pit</span>
    <span class="n">state</span><span class="p">[</span><span class="n">pit</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1">#re-place wall</span>
    <span class="n">state</span><span class="p">[</span><span class="n">wall</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1">#re-place goal</span>
    <span class="n">state</span><span class="p">[</span><span class="n">goal</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">state</span>
</code></pre></div>

<p>The first thing we do is try to find the positions of each element on the grid (state). Then it's just a few simple if-conditions. We need to make sure the player isn't trying to step on the wall and make sure that the player isn't stepping outside the bounds of the grid.</p>
<p>Now we implement <code>getLoc</code> which is similar to <code>findLoc</code> but can identify superimposed elements, whereas <code>findLoc</code> would miss it (intentionally) if there was superimposition. Additionally, we'll implement our reward function, which will award +10 if the player steps onto the goal, -10 if the player steps into the pit, and -1 for any other move. These rewards are pretty arbitrary, as long as the goal has a significantly higher reward than the pit, the algorithm should do fine.</p>
<p>Lastly, I've implemented a function that will display our grid as a text array so we can see what's going on.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">getLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">level</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">][</span><span class="n">level</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span>

<span class="k">def</span> <span class="nf">getReward</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">player_loc</span> <span class="o">=</span> <span class="n">getLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">pit</span> <span class="o">=</span> <span class="n">getLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">goal</span> <span class="o">=</span> <span class="n">getLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">player_loc</span> <span class="o">==</span> <span class="n">pit</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">10</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">player_loc</span> <span class="o">==</span> <span class="n">goal</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">10</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>

<span class="k">def</span> <span class="nf">dispGrid</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;&lt;U2&#39;</span><span class="p">)</span>
    <span class="n">player_loc</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">wall</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">goal</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">pit</span> <span class="o">=</span> <span class="n">findLoc</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">grid</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span>

    <span class="k">if</span> <span class="n">player_loc</span><span class="p">:</span>
        <span class="n">grid</span><span class="p">[</span><span class="n">player_loc</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;P&#39;</span> <span class="c1">#player</span>
    <span class="k">if</span> <span class="n">wall</span><span class="p">:</span>
        <span class="n">grid</span><span class="p">[</span><span class="n">wall</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;W&#39;</span> <span class="c1">#wall</span>
    <span class="k">if</span> <span class="n">goal</span><span class="p">:</span>
        <span class="n">grid</span><span class="p">[</span><span class="n">goal</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;+&#39;</span> <span class="c1">#goal</span>
    <span class="k">if</span> <span class="n">pit</span><span class="p">:</span>
        <span class="n">grid</span><span class="p">[</span><span class="n">pit</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;-&#39;</span> <span class="c1">#pit</span>

    <span class="k">return</span> <span class="n">grid</span>
</code></pre></div>

<p>And that's it. That's the entire gridworld game implementation. Not too bad right? As with my part 2 blackjack implementation, this game is not using OOP-style and implemented in a functional style where we just pass around states.</p>
<p>Let's demonstrate some gameplay. I'll be using the <code>initGridRand()</code> variant so that all items are placed randomly.</p>
<div class="highlight"><pre><span></span><code><span class="n">state</span> <span class="o">=</span> <span class="n">initGridRand</span><span class="p">()</span>
<span class="n">dispGrid</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[&#39;P&#39;, &#39;-&#39;, &#39; &#39;, &#39; &#39;],</span>
<span class="err">       [&#39; &#39;, &#39; &#39;, &#39; &#39;, &#39; &#39;],</span>
<span class="err">       [&#39; &#39;, &#39; &#39;, &#39;W&#39;, &#39; &#39;],</span>
<span class="err">       [&#39; &#39;, &#39;+&#39;, &#39; &#39;, &#39; &#39;]], </span>
<span class="err">      dtype=&#39;&lt;U2&#39;)</span>
</code></pre></div>

<p>As you can see, I clearly need to move 3 spaces down, and 1 space to the right to land on the goal.
Remember, our action encoding is: 0 = up, 1 = down, 2 = left, 3 = right.</p>
<div class="highlight"><pre><span></span><code><span class="n">state</span> <span class="o">=</span> <span class="n">makeMove</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">makeMove</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">makeMove</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">makeMove</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Reward: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">getReward</span><span class="p">(</span><span class="n">state</span><span class="p">),))</span>
<span class="n">dispGrid</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">Reward</span><span class="o">:</span> <span class="mi">10</span>





<span class="n">array</span><span class="o">([[</span><span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39;-&#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">],</span>
       <span class="o">[</span><span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">],</span>
       <span class="o">[</span><span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39;W&#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">],</span>
       <span class="o">[</span><span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">,</span> <span class="s1">&#39; &#39;</span><span class="o">]],</span> 
      <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;&lt;U2&#39;</span><span class="o">)</span>
</code></pre></div>

<p>We haven't implemented a display for when the player is on the goal or pit so the player and goal just disappear when that happens. </p>
<h3>Neural Network as our Q function</h3>
<p>Now for the fun part. Let's build our neural network that will serve as our <span class="math">\(Q\)</span> function. Since this is a post about Q-learning, I'm not going to code a neural network from scratch. I'm going to use the fairly popular Theano-based library Keras. You can of course use whatever library you want, or roll your own.</p>
<p><strong>Important Note</strong>:
Up until now, I've been talking about how the neural network can serve the role of <span class="math">\(Q(s, a)\)</span>, and that's absolutely true. However, I will be implementing our neural network in the same way that Google DeepMind did for its Atari playing algorithm. Instead of a neural network architecture that accepts a state and an action as inputs and outputs the value of that single state-action pair, DeepMind built a network that just accepts a state and outputs separate Q-values for each possible action in its output layer. This is pretty clever because in Q-learning we need to get the <span class="math">\(maxQ(s', a')\)</span> [max of the Q values for every possible action in the new state s']. Rather than having to run our network forward for every action, we just need to run it forward once. The result is the same, however, it's just more efficient.</p>
<p><img src="images/RL/rl3net.png" /></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">RMSprop</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">164</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="c1">#model.add(Dropout(0.2)) I&#39;m not using dropout, but maybe you wanna give it a try?</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="c1">#model.add(Dropout(0.2))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span> <span class="c1">#linear output so we can have range of real-valued outputs</span>

<span class="n">rms</span> <span class="o">=</span> <span class="n">RMSprop</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">rms</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#just to show an example output; read outputs left to right: up/down/left/right</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[-0.02812552, -0.04649779, -0.08819015, -0.00723661]])</span>
</code></pre></div>

<p>So that's the network I've designed. An input layer of 64 units (because our state has a total of 64 elements, remember its a 4x4x4 numpy array), 2 hidden layers of 164 and 150 units, and an output layer of 4, one for each of our possible actions (up, down, left, right) [in that order].</p>
<p>Why did I make the network like this? Honestly, I have no good answer for that. I just messed around with different hidden layer architectures and this one seemed to work fairly well. Feel free to change it up. There's probably a better configuration. (If you discover or know of a much better network architecture for this, let me know).</p>
<h3>Online Training</h3>
<p>Below is the implementation for the main loop of the algorithm. In broad strokes:
1. Setup a for-loop to number of epochs
2. In the loop, setup while loop (while game is in progress)
3. Run Q network forward.
4. We're using an epsilon greedy implementation, so at time <em>t</em> with probability <span class="math">\(\epsilon\)</span> we will choose a random action. With probability <span class="math">\(1-\epsilon\)</span> we will choose the action associated with the highest Q value from our neural network.
5. Take action <span class="math">\(a\)</span> as determined in (4), observe new state <span class="math">\(s'\)</span> and reward <span class="math">\(r_{t+1}\)</span>
6. Run the network forward using <span class="math">\(s'\)</span>. Store the highest Q value (<code>maxQ</code>).
7. Our target value to train the network is <code>reward + (gamma * maxQ)</code> where <code>gamma</code> is a parameter (<span class="math">\(0 &lt;= \gamma &lt;= 1\)</span>).
8. Given that we have 4 outputs and we only want to update/train the output associated with the action we just took, our target output vector is the same as the output vector from the first run, except we change the one output associated with our action to: <code>reward + (gamma * maxQ)</code>
9. Train the model on this 1 sample. Repeat process 2-9</p>
<p>Just to be clear, when we first run our neural network and get an output of action-values like this</p>
<div class="highlight"><pre><span></span><code><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.02812552</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04649779</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.08819015</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00723661</span><span class="p">]])</span>
</code></pre></div>

<p>our target vector for one iteration may look like this:</p>
<div class="highlight"><pre><span></span><code><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.02812552</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04649779</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00723661</span><span class="p">]])</span>
</code></pre></div>

<p>Also note, I initialize epsilon (for the <span class="math">\(\epsilon\)</span>-greedy action selection) to be 1. It decrements by a small amount on every iteration and will eventually reach 0.1 where it stays. Google DeepMind also used an <span class="math">\(\epsilon\)</span>-greedy action selection and also initialized epsilon to be 1 and decremented during the game play.
if taking action 2 one step (left) resulted in reaching the goal. So we just keep all other outputs the same as before and just change the one for the action we took.</p>
<p>Okay, so let's go ahead and train our algorithm to learn the easiest variant of the game, where all elements are placed deterministically at the same positions every time.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="c1">#since it may take several moves to goal, making gamma high</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">initGrid</span><span class="p">()</span>
    <span class="n">status</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1">#while game still in progress</span>
    <span class="k">while</span><span class="p">(</span><span class="n">status</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1">#We are in state S</span>
        <span class="c1">#Let&#39;s run our Q function on S to get Q values for all possible actions</span>
        <span class="n">qval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">):</span> <span class="c1">#choose random action</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1">#choose best action from Q(s,a) values</span>
            <span class="n">action</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">qval</span><span class="p">))</span>
        <span class="c1">#Take action, observe new state S&#39;</span>
        <span class="n">new_state</span> <span class="o">=</span> <span class="n">makeMove</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="c1">#Observe reward</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">getReward</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>
        <span class="c1">#Get max_Q(S&#39;,a)</span>
        <span class="n">newQ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">maxQ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">newQ</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
        <span class="n">y</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">qval</span><span class="p">[:]</span>
        <span class="k">if</span> <span class="n">reward</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span> <span class="c1">#non-terminal state</span>
            <span class="n">update</span> <span class="o">=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">maxQ</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1">#terminal state</span>
            <span class="n">update</span> <span class="o">=</span> <span class="n">reward</span>
        <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">update</span> <span class="c1">#target output</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Game #: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,))</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
        <span class="k">if</span> <span class="n">reward</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&gt;</span> <span class="mf">0.1</span><span class="p">:</span>
        <span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">epochs</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">Game #: 999</span>
<span class="err">Epoch 1/1</span>
<span class="err">1/1 [==============================] - 0s - loss: 0.0265</span>
</code></pre></div>

<p>Alright, so I've empirically tested this and it trains on the easy variant with just 1000 epochs (keep in mind every epoch is a full game played to completion). Below I've implemented a function we can use to test our trained algorithm to see if it has properly learned how to play the game. It basically just uses the neural network model to calculate action-values for the current state and selects the action with the highest Q-value. It just repeats this forever until the game is won or lost. I've made it break out of this loop if it is making more than 10 moves because this probably means it hasn't learned how to win and we don't want an infinite loop running.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">testAlgo</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">init</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">initGrid</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">init</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">initGridPlayer</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">init</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">initGridRand</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initial State:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dispGrid</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
    <span class="n">status</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1">#while game still in progress</span>
    <span class="k">while</span><span class="p">(</span><span class="n">status</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">qval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">qval</span><span class="p">))</span> <span class="c1">#take action with highest Q-value</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Move #: </span><span class="si">%s</span><span class="s1">; Taking action: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">action</span><span class="p">))</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">makeMove</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dispGrid</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">getReward</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reward</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">reward</span><span class="p">,))</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1">#If we&#39;re taking more than 10 actions, just stop, we probably can&#39;t win this game</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Game lost; too many moves.&quot;</span><span class="p">)</span>
            <span class="k">break</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">testAlgo</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">Initial State:</span>
<span class="err">[[&#39; &#39; &#39;P&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39;+&#39;]]</span>
<span class="err">Move #: 0; Taking action: 3</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39;P&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39;+&#39;]]</span>
<span class="err">Move #: 1; Taking action: 3</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39;P&#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39;+&#39;]]</span>
<span class="err">Move #: 2; Taking action: 1</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39; &#39; &#39;P&#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39;+&#39;]]</span>
<span class="err">Move #: 3; Taking action: 1</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39;P&#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39;+&#39;]]</span>
<span class="err">Move #: 4; Taking action: 1</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="c">Reward: 10</span>
</code></pre></div>

<p>Can we get a round of applause for our gridworld player here? Clearly it knows what its doing; it went straight for the prize!</p>
<h3>Playing the the harder variant, catastrophic forgetting, and experience replay</h3>
<p>We're slowly building up our chops and we want our algorithm to train on the harder variant of the game where every new game the player is randomly placed on the grid. It can't just memorize a sequence of steps to take as before, it needs to be able to take the shortest path to the goal (without stepping into the pit) from wherever it starts on the grid. It needs to develop a slightly more sophisticated representation of its environment.</p>
<p>Unfortunately, there is a problem we may need to deal with as our problem becomes increasingly more difficult. There is a known problem called <strong>catastrophic forgetting</strong> that is associated with gradient descent based training methods in online training. </p>
<p>Imagine that in game #1 that our algorithm is training on (learning Q-values for) the player is placed in between the pit and the goal such that the goal is on the right and the pit is on the left. Using epsilon-greedy strategy, the player takes a random move and by chance takes a step to the right and hits the goal. Great, the algorithm will try to learn that this state-action pair is associated with a high reward by updating its weights in such a way that the output will more closely match the target value (i.e backpropagation). Now, the second game gets initialized and the player is again in between the goal and pit but this time the goal is on the <em>left</em> and the pit is on the right. Perhaps to our naive algorithm, the state <em>seems</em> very similar to the last game.  Let's say that again, the player chooses to make one step to the right, but this time it ends up in the pit and gets -10 reward. The player is thinking "what the hell I thought going to the right was the best decision based on my previous experience." So now it may do backpropagation again to update its state-action value but because this state-action is very similar to the last learned state-action it may mess up its previously learned weights. </p>
<p>This is the essence of catastrophic forgetting. There's a push-pull between very similar state-actions (but with divergent targets) that results in this inability to properly learn anything. We generally don't have this problem in the supervised learning realm because we do randomized batch learning, where we don't update our weights until we've iterated through some random subset of our training data.</p>
<p>Catastrophic forgetting is probably not something we have to worry about with the first variant of our game because the targets are always stationary; but with the harder variants, it's something we should consider, and that is why I'm implementing something called <strong>experience replay</strong>. Experience replay basically gives us minibatch updating in an online learning scheme. It's actually not a huge deal to implement; here's how it works.</p>
<p>Experience replay:
1. In state <span class="math">\(s\)</span>, take action <span class="math">\(a\)</span>, observe new state <span class="math">\(s_{t+1}\)</span> and reward <span class="math">\(r_{t+1}\)</span>
2. Store this as a tuple <span class="math">\((s, a, s_{t+1}, r_{t+1})\)</span> in a list.
3. Continue to store each experience in this list until we have filled the list to a specific length (up to you to define)
4. Once the experience replay memory is filled, randomly select a subset (e.g. 40)
5. Iterate through this subset and calculate value updates for each; store these in a target array (e.g. <code>y_train</code>) and store the state <span class="math">\(s\)</span> of each memory in <code>X_train</code>
6. Use <code>X_train</code> and <code>y_train</code> as a minibatch for batch training. For subsequent epochs where the array is full, just overwrite old values in our experience replay memory array.</p>
<p>Thus, in addition to learning the action-value for the action we just took, we're also going to use a random sample of our past experiences to train on to prevent catastrophic forgetting.</p>
<p>So here's the same training algorithm from above except with experience replay added. Remember, this time we're training it on the harder variant of the game where the player is randomly placed on the grid.</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">rms</span><span class="p">)</span><span class="c1">#reset weights of neural network</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.975</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batchSize</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">buffer</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">replay</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#stores tuples of (S, A, R, S&#39;)</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">initGridPlayer</span><span class="p">()</span> <span class="c1">#using the harder state initialization function</span>
    <span class="n">status</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1">#while game still in progress</span>
    <span class="k">while</span><span class="p">(</span><span class="n">status</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1">#We are in state S</span>
        <span class="c1">#Let&#39;s run our Q function on S to get Q values for all possible actions</span>
        <span class="n">qval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">):</span> <span class="c1">#choose random action</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1">#choose best action from Q(s,a) values</span>
            <span class="n">action</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">qval</span><span class="p">))</span>
        <span class="c1">#Take action, observe new state S&#39;</span>
        <span class="n">new_state</span> <span class="o">=</span> <span class="n">makeMove</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="c1">#Observe reward</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">getReward</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>

        <span class="c1">#Experience replay storage</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">replay</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">buffer</span><span class="p">):</span> <span class="c1">#if buffer not filled, add to it</span>
            <span class="n">replay</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1">#if buffer full, overwrite old values</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">h</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">buffer</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
                <span class="n">h</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">replay</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">)</span>
            <span class="c1">#randomly sample our experience replay memory</span>
            <span class="n">minibatch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">replay</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">)</span>
            <span class="n">X_train</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">y_train</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">memory</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">:</span>
                <span class="c1">#Get max_Q(S&#39;,a)</span>
                <span class="n">old_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="n">memory</span>
                <span class="n">old_qval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">old_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">newQ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">maxQ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">newQ</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
                <span class="n">y</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">old_qval</span><span class="p">[:]</span>
                <span class="k">if</span> <span class="n">reward</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span> <span class="c1">#non-terminal state</span>
                    <span class="n">update</span> <span class="o">=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">maxQ</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span> <span class="c1">#terminal state</span>
                    <span class="n">update</span> <span class="o">=</span> <span class="n">reward</span>
                <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">update</span>
                <span class="n">X_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">old_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">64</span><span class="p">,))</span>
                <span class="n">y_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,))</span>

            <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
            <span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Game #: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,))</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
        <span class="k">if</span> <span class="n">reward</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span> <span class="c1">#if reached terminal state, update game status</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&gt;</span> <span class="mf">0.1</span><span class="p">:</span> <span class="c1">#decrement epsilon over time</span>
        <span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">epochs</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">Game #: 2999</span>
<span class="err">Epoch 1/1</span>
<span class="err">40/40 [==============================] - 0s - loss: 0.0018</span>
</code></pre></div>

<p>I've increased the training epochs to 3000 just based on empiric testing. So let's see how it does, we'll run our <code>testAlgo()</code> function a couple times to see how it handles randomly initialized player scenarios.</p>
<div class="highlight"><pre><span></span><code><span class="n">testAlgo</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#run testAlgo using random player placement =&gt; initGridPlayer()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">Initial State:</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;P&#39; &#39; &#39;]]</span>
<span class="err">Move #: 0; Taking action: 3</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39;P&#39;]]</span>
<span class="err">Move #: 1; Taking action: 0</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39;P&#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="err">Move #: 2; Taking action: 0</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39;P&#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="err">Move #: 3; Taking action: 2</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="c">Reward: 10</span>
</code></pre></div>

<p>Fantastic. Let's run the <code>testAlgo()</code> one more time just to prove it has generalized.</p>
<div class="highlight"><pre><span></span><code><span class="n">testAlgo</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#Of course, I ran it many times more than I&#39;m showing here</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">Initial State:</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;P&#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="err">Move #: 0; Taking action: 2</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39;P&#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="err">Move #: 1; Taking action: 0</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39;P&#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="err">Move #: 2; Taking action: 0</span>
<span class="err">[[&#39;P&#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="err">Move #: 3; Taking action: 3</span>
<span class="err">[[&#39; &#39; &#39;P&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="err">Move #: 4; Taking action: 3</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39;P&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39;+&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="err">Move #: 5; Taking action: 1</span>
<span class="err">[[&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39;-&#39; &#39; &#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39;W&#39; &#39; &#39;]</span>
<span class="err"> [&#39; &#39; &#39; &#39; &#39; &#39; &#39; &#39;]]</span>
<span class="c">Reward: 10</span>
</code></pre></div>

<p>I'll be darned. It seems to have learned to play the game from any starting position! Pretty neat.</p>
<h3>The Hardest Variant</h3>
<p>Okay, I lied. I will not be showing you the algorithm learning the hardest variant of the game (where all 4 elements are randomly placed on the grid each game). I'm leaving that up to you to attempt and let me know how it goes via email (outlacedev@gmail.com). The reason is, I'm doing all this on a Macbook Air (read: no CUDA gpu) and thus I cannot train the algorithm to a sufficiently large number of epochs for it to learn the problem. I suspect it may require significantly more epochs, perhaps more than 50,000. So if you have an nVIDIA GPU and can train it that long, let me know if it works. I could have used Lua/Torch7 since there is an OpenCL version but no one would read this if it wasn't in Python =P.</p>
<h3>Conclusion</h3>
<p>There you have it, basic Q-learning using neural networks.</p>
<p>That was a lot to go through, hopefully I didn't make too many mistakes (as always, email if you spot any so I can post corrections). I'm hoping you have success training Q-learning algorithms on more interesting problems than the gridworld game.</p>
<p>I'd say this is definitely the climax of the series on reinforcement learning. I plan to release a part 4 that will be about other temporal difference learnings algorithms that use eligibility traces. Since that's a relatively minor new concept, I will likely use it on another toy problem like gridworld. However, I do, at some point, want to release a post about setting up and using the Arcade Learning Environment (ALE) [fmr. Atari Learning Environment] and training an alogorithm to play Atari games, however, that will likely be a long while from now so don't hold your breath.</p>
<p>Cheers</p>
<h3>Download this IPython Notebook</h3>
<p><a href="https://github.com/outlace/outlace.github.io/blob/master/notebooks/rlpart3.ipynb">https://github.com/outlace/outlace.github.io/blob/master/notebooks/rlpart3.ipynb</a></p>
<h3>Download the Gridworld Game</h3>
<p><a href="https://github.com/outlace/Gridworld">https://github.com/outlace/Gridworld</a></p>
<h3>References</h3>
<ol>
<li>http://www.computervisiontalks.com/deep-learning-lecture-16-reinforcement-learning-and-neuro-dynamic-programming-nando-de-freitas/</li>
<li>https://www.youtube.com/watch?v=yNeSFbE1jdY</li>
<li>http://www.researchgate.net/profile/Marco_Wiering/publication/236645821_Reinforcement_Learning_to_Train_Ms._Pac-Man_Using_Higher-order_Action-relative_Inputs/links/0deec518a22042f5d7000000.pdf?inViewer=true&amp;pdfJsDownload=true&amp;disableCoverPage=true&amp;origin=publication_detail</li>
<li>"Reinforcement Learning An Introduction" Sutton &amp; Barto, 1996</li>
<li>"Human-level control through deep reinforcement learning" Mnih et al, 2015 (Google DeepMind Atari paper)</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <div class="clear"></div>

                <div class="info">
                    <a href="http://outlace.com/rlpart3.html">posted at 00:00</a>
                    by Brandon Brown
                    &nbsp;&middot;&nbsp;<a href="http://outlace.com/category/reinforcement-learning/" rel="tag">Reinforcement-Learning</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://outlace.com/tag/q-learning/" class="tags">Q-learning</a>
                    &nbsp;<a href="http://outlace.com/tag/rl/" class="tags">RL</a>
                </div>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'outlace';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
            </article>
            <div class="clear"></div>
            <footer>
                <p>
                <!--- <a href="http://outlace.com/feeds/all.atom.xml" rel="alternate">Atom Feed</a> --->
                <a href="mailto:outlacedev@gmail.com"><i class="svg-icon email"></i></a>
                <a href="http://github.com/outlace"><i class="svg-icon github"></i></a>
                <a href="http://outlace.com/feeds/all.atom.xml"><i class="svg-icon rss"></i></a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-65814776-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
</body>
</html>