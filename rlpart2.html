<!DOCTYPE html>
<html lang="en">
<head>
    
        <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Δ ℚuantitative √ourney | Reinforcement Learning - Monte Carlo Methods</title>
    <link rel="shortcut icon" type="image/png" href="http://outlace.com/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="http://outlace.com/favicon.ico">
    <link href="http://outlace.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Δ ℚuantitative √ourney Full Atom Feed" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Brandon Brown" />

    <meta name="keywords" content="Monte-Carlo,RL" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="http://outlace.com/">Home</a></li>
                <li><a href="http://outlace.com/pages/about.html">About</a></li>
                <li><a href="http://outlace.com/tags/">Tags</a></li>
                <li><a href="http://outlace.com/categories/">Categories</a></li>
                <li><a href="http://outlace.com/archives/{slug}/">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="http://outlace.com/">Δ ℚuantitative √ourney</a></h1>
            <h2>∑ Our experiences in learning quantitative applications</h2>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Oct 25, 2015</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://outlace.com/rlpart2.html" rel="bookmark" title="Permanent Link to &quot;Reinforcement Learning - Monte Carlo Methods&quot;">Reinforcement Learning - Monte Carlo Methods</a>
                </h2>



                <h3>Playing Blackjack with Monte Carlo Methods</h3>
<h5>Introduction</h5>
<p>In part 1, we considered a very simple problem, the n-armed bandit problem, and devised an appropriately very simple algorithm to solve it (<span class="math">\(\epsilon\)</span>-greedy evaluation). In that case, the problem only has a single state: a choice among 10 actions with stationary probability distributions of rewards. Let's up the ante a bit and consider a more interesting problem with multiple (yet finite) states: the card game black jack (aka 21). Hunker down, this is a long one.</p>
<p>Rules and game-play of blackjack (check out https://www.youtube.com/watch?v=qd5oc9hLrXg if necessary):
1. There is a dealer and 1 or more players that independently play against the dealer.
2. Each player is delt 2 cards face-up. The dealer is delt two cards, one face-up, one face-down.
3. The goal is to get the sum of your cards value to be as close to 21 as possible without going over.
4. After the initial cards are dealt, each player can choose to 'stay' or 'hit' (ask for another card).
5. The dealer always follows this policy: hit until cards sum to 17 or more, then stay.
6. If the dealer is closer to 21, the dealer wins and the player loses, and vice versa.</p>
<p>So what's the state space for this problem? It's relatively large, much much larger than the single state in n-armed bandit. In reinforcement learning, a state is all information available to the agent (the decision maker) at a particular time <span class="math">\(t\)</span>. The reason why the n-armed bandit state space includes just 1 state is because the agent is only aware of the same 10 actions at any time, no new information is available nor do the actions change.</p>
<p>So what are all the possible combinations of information available to the agent (the player) in blackjack? Well, the player starts with two cards, so there is the combination of all 2 playing cards. Additionally, the player knows one of the two cards that the dealer has. Thus, there are a lot of possible states (around 200). As with any RL problem, our ultimate goal is to find the best <em>policy</em> to maximize our rewards. </p>
<p>A policy is roughly equivalent to a strategy. There are reinforcement learning methods that essentially rely on brute force to compute every possible action-state pair (every possible action in a given state) and the rewards received to find an optimal policy, but for most of the problems we care about, the state-action space is much too large for brute force methods to be computationally feasible. Thus we must rely on experience, i.e. playing the game, trying out various actions and learning what seems to result in the greatest reward returns; and we need to devise an algorithm that captures this experiential learning process.</p>
<p>The most important take-aways from part 1 and 2 are the concepts of state values, state-action values, and policies. Reinforcement learning is in the business of determining the value of states or of actions taken in a state. In our case, we will primarily concern ourselves with action values (value of an action taken in a given state) because it is more intuitive in how we can make an optimal action. I find the value of being in a given state less intuitive because the value of a state depends on your policy. For example, what is the value of being in a state of a blackjack game where your cards total to 20? Most people would say that's a pretty good position to be in, but it's only a good state if your policy is to stay and not hit. If your policy is to hit when you have 20 (of course it's a bad policy), then that state isn't very good. On the other hand, we can ask the question of, what's the value of hitting when I have 20 versus the value of staying when I have 20, and then just choose whichever action has the highest value. Of course staying would produce the highest value in this state (on average).</p>
<p>Our main computational effort, therefore, is in iteratively improving our estimates for the values of states or state-action pairs. In parts 1 and 2, we keep track of every single state-action pair we encounter, and record the rewards we receive for each and average them over time. Thus, over many iterations, we go from knowing nothing about the value of state-actions to knowing enough to be able to choose the highest value actions. Problems like the n-armed bandit problem and blackjack have a small enough state or state-action space that we can record and average rewards in a lookup table, giving us the exact average rewards for each state-action pair. Most interesting problems, however, have a state space that is continuous or otherwise too large to use a lookup table. That's when we must use function approximation (e.g. neural networks) methods to serve as our <span class="math">\(Q\)</span> function in determining the value of states or state-actions.  We will have to wait for part 3 for neural networks.</p>
<h4>Learning with Markov Decision Processes</h4>
<p>A Markov decision process (MDP) is a decision that can be made knowing only the current state, without knowledge of or reference to previous states or the path taken to the current state. That is, the current state contains enough information to choose optimal actions to maximize future rewards. Most RL algorithms assume that the problems to be learned are (at least approximately) Markov decision processes. Blackjack is clearly an MDP because we can play the game successfully by just knowing our current state (i.e. what cards we have + the dealer's one face-up card). Google DeepMind's deep Q-learning algorithm learned to play Atari games from just raw pixel data and the current score. Does raw pixel data and the score satisfy the Markov property? Not exactly. Say the game is Pacman, if our state is the raw pixel data from our current frame, we have no idea if that enemy a few tiles away is approaching us or moving away from us, and that would strongly influence our choice of actions to take. This is why DeepMind's implementation actually feeds in the last 4 frames of gameplay, effectively changing a non-Markov decision process into an MDP. With the last 4 frames, the agent has access to the direction and speed of each enemy (and itself).</p>
<h4>Terminology &amp; Notation Review</h4>
<ol>
<li><span class="math">\(Q_k(s, a)\)</span> is the function that accepts an action and state and returns the value of taking that action in that state at time step <span class="math">\(k\)</span>. This is fundamental to RL. We need to know the relative values of every state or state-action pair.</li>
<li><span class="math">\(\pi\)</span> is a policy, a stochastic strategy or rule to choose action <span class="math">\(a\)</span> given a state <span class="math">\(s\)</span>. Think of it as a function, <span class="math">\(\pi(s)\)</span>, that accepts state, <span class="math">\(s\)</span> and returns the action to be taken. There is a distinction between the <span class="math">\(\pi(s)\)</span> <em>function</em> and a specific policy <span class="math">\(\pi\)</span>. Our implementation of <span class="math">\(\pi(s)\)</span> as a function is often to just choose the action <span class="math">\(a\)</span> in state <span class="math">\(s\)</span> that has the highest average return based on historical results, <span class="math">\(argmaxQ(s,a)\)</span>. As we gather more data and these average returns become more accurate, the actual policy <span class="math">\(\pi\)</span> may change. We may start out with a policy of "hit until total is 16 or more then stay" but this policy may change as we gather more data. Our implemented <span class="math">\(\pi(s)\)</span> function, however, is programmed by us and does not change.</li>
<li><span class="math">\(G_t\)</span>, return. The expected cumulative reward from starting in a given state until the end of an episode (i.e. game play), for example. In our case we only give a reward at the end of the game, there are no rewards at each time step or move.</li>
<li>Episode: The full sequence of steps leading to a terminal state and receiving a return. E.g. from the beginning of a blackjack game until the terminal state (someone winning) constitutes an episode of play.</li>
<li><span class="math">\(v_\pi\)</span>, a function that determines the value of a state given a policy <span class="math">\(\pi\)</span>. We do not really concern our selves with state values here, we focus on action values.</li>
</ol>
<h3>Monte Carlo &amp; Tabular Methods</h3>
<p>Monte Carlo is going to feel very familiar to how we solved the n-armed bandit problem from part 1. We will store the history of our state-action pairs associated with their values in a table, and then refer to this table during learning to calculate our expected rewards, <span class="math">\(Q_k\)</span>.</p>
<p>From Wikipedia, Monte Carlo methods "rely on repeated random sampling to obtain numerical results." We'll use random sampling of states and state-action pairs and observe rewards and then iteratively revise our policy, which will hopefully converge on the optimal policy as we explore every possible state-action couple.</p>
<p>Here are some important points:</p>
<ol>
<li>We will asign a reward of +1 to winning a round of blackjack, -1 for losing, and 0 for a draw.</li>
<li>We will establish a table (python dictionary) where each key corresponds to a particular state-action pair and each value is the value of that pair. i.e. the average reward received for that action in that state.</li>
<li>The state consists of the player's card total, whether or not the player has a useable ace, and the dealer's one face-up card</li>
</ol>
<h3>Blackjack Game Implementation</h3>
<p>Below I've implemented a blackjack game. I think I've commented it well enough to be understood but it's not critical that you understand the game implementation since we're just concerned with how to learn to play the game with machine learning.</p>
<p>This implementation is completely functional and stateless. I mean that this implementation is just a group of functions that accept data, transform that data and return new data. I intentionally avoided using OOP classes because I think it complicates things and I think functional-style programming is useful in machine learning (see my post about computational graphs to learn more). It is particularly useful in our case because it demonstrates how blackjack is an MDP. The game does not store any information, it is stateless. It merely accepts states and returns new states. The player is responsible for saving states if they want.</p>
<p>The state is just a Python tuple where the first element is the player's card total, the 2nd element is a boolean of whether or not the player has a useable ace. The 3rd element is the card total for the dealer and then another boolean of whether or not its a useable ace. The last element is a single integer that represents the status of the state (whether the game is in progress, the player has won, the dealer has won, or it was a draw).</p>
<p>We actually could implement this in a more intuitive way where we just store each player's cards and not whether or not they have a useable ace (useable means, can the ace be an 11 without losing the game by going over 21, because aces in blackjack can either be a 1 or an 11). However, as you'll see, storing the player card total and an useable ace boolean is equivalent and yet compresses our state space (without losing any information) so we can have a smaller lookup table.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="c1">#each value card has a 1:13 chance of being selected (we don&#39;t care about suits for blackjack)</span>
<span class="c1">#cards (value): Ace (1), 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack (10), Queen (10), King (10)</span>

<span class="k">def</span> <span class="nf">randomCard</span><span class="p">():</span>
    <span class="n">card</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">13</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">card</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">card</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">return</span> <span class="n">card</span>

<span class="c1">#A hand is just a tuple e.g. (14, False), a total card value of 14 without a useable ace</span>
<span class="c1">#accepts a hand, if the Ace can be an 11 without busting the hand, it&#39;s useable</span>
<span class="k">def</span> <span class="nf">useable_ace</span><span class="p">(</span><span class="n">hand</span><span class="p">):</span>
    <span class="n">val</span><span class="p">,</span> <span class="n">ace</span> <span class="o">=</span> <span class="n">hand</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">ace</span><span class="p">)</span> <span class="ow">and</span> <span class="p">((</span><span class="n">val</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">21</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">totalValue</span><span class="p">(</span><span class="n">hand</span><span class="p">):</span>
    <span class="n">val</span><span class="p">,</span> <span class="n">ace</span> <span class="o">=</span> <span class="n">hand</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">useable_ace</span><span class="p">(</span><span class="n">hand</span><span class="p">)):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">val</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">val</span>

<span class="k">def</span> <span class="nf">add_card</span><span class="p">(</span><span class="n">hand</span><span class="p">,</span> <span class="n">card</span><span class="p">):</span>
    <span class="n">val</span><span class="p">,</span> <span class="n">ace</span> <span class="o">=</span> <span class="n">hand</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">card</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">ace</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">val</span> <span class="o">+</span> <span class="n">card</span><span class="p">,</span> <span class="n">ace</span><span class="p">)</span>
<span class="c1">#The first is first dealt a single card, this method finishes off his hand</span>
<span class="k">def</span> <span class="nf">eval_dealer</span><span class="p">(</span><span class="n">dealer_hand</span><span class="p">):</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">totalValue</span><span class="p">(</span><span class="n">dealer_hand</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">17</span><span class="p">):</span>
        <span class="n">dealer_hand</span> <span class="o">=</span> <span class="n">add_card</span><span class="p">(</span><span class="n">dealer_hand</span><span class="p">,</span> <span class="n">randomCard</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">dealer_hand</span>

<span class="c1">#state: (player total, useable_ace), (dealer total, useable ace), game status; e.g. ((15, True), (9, False), 1)</span>
<span class="c1">#stay or hit =&gt; dec == 0 or 1</span>
<span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dec</span><span class="p">):</span>
    <span class="c1">#evaluate</span>
    <span class="n">player_hand</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#val, useable ace</span>
    <span class="n">dealer_hand</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">dec</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#action = stay</span>
        <span class="c1">#evaluate game; dealer plays</span>
        <span class="n">dealer_hand</span> <span class="o">=</span> <span class="n">eval_dealer</span><span class="p">(</span><span class="n">dealer_hand</span><span class="p">)</span>

        <span class="n">player_tot</span> <span class="o">=</span> <span class="n">totalValue</span><span class="p">(</span><span class="n">player_hand</span><span class="p">)</span>
        <span class="n">dealer_tot</span> <span class="o">=</span> <span class="n">totalValue</span><span class="p">(</span><span class="n">dealer_hand</span><span class="p">)</span>
        <span class="n">status</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">dealer_tot</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="p">):</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1">#player wins</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">dealer_tot</span> <span class="o">==</span> <span class="n">player_tot</span><span class="p">):</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1">#draw</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">dealer_tot</span> <span class="o">&lt;</span> <span class="n">player_tot</span><span class="p">):</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1">#player wins</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">dealer_tot</span> <span class="o">&gt;</span> <span class="n">player_tot</span><span class="p">):</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1">#player loses</span>

    <span class="k">elif</span> <span class="n">dec</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c1">#action = hit</span>
        <span class="c1">#if hit, add new card to player&#39;s hand</span>
        <span class="n">player_hand</span> <span class="o">=</span> <span class="n">add_card</span><span class="p">(</span><span class="n">player_hand</span><span class="p">,</span> <span class="n">randomCard</span><span class="p">())</span>
        <span class="n">d_hand</span> <span class="o">=</span> <span class="n">eval_dealer</span><span class="p">(</span><span class="n">dealer_hand</span><span class="p">)</span>
        <span class="n">player_tot</span> <span class="o">=</span> <span class="n">totalValue</span><span class="p">(</span><span class="n">player_hand</span><span class="p">)</span>
        <span class="n">status</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">player_tot</span> <span class="o">==</span> <span class="mi">21</span><span class="p">):</span> 
            <span class="k">if</span> <span class="p">(</span><span class="n">totalValue</span><span class="p">(</span><span class="n">d_hand</span><span class="p">)</span> <span class="o">==</span> <span class="mi">21</span><span class="p">):</span>
                <span class="n">status</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1">#draw</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">status</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1">#player wins!</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">player_tot</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="p">):</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1">#player loses</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">player_tot</span> <span class="o">&lt;</span> <span class="mi">21</span><span class="p">):</span>
            <span class="c1">#game still in progress</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">player_hand</span><span class="p">,</span> <span class="n">dealer_hand</span><span class="p">,</span> <span class="n">status</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">state</span>

<span class="c1">#start a game of blackjack, returns a random initial state</span>
<span class="k">def</span> <span class="nf">initGame</span><span class="p">():</span>
    <span class="n">status</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1">#1=in progress; 2=player won; 3=draw; 4 = dealer won/player loses</span>
    <span class="n">player_hand</span> <span class="o">=</span> <span class="n">add_card</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span> <span class="n">randomCard</span><span class="p">())</span>
    <span class="n">player_hand</span> <span class="o">=</span> <span class="n">add_card</span><span class="p">(</span><span class="n">player_hand</span><span class="p">,</span> <span class="n">randomCard</span><span class="p">())</span>
    <span class="n">dealer_hand</span> <span class="o">=</span> <span class="n">add_card</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span> <span class="n">randomCard</span><span class="p">())</span>
    <span class="c1">#evaluate if player wins from first hand</span>
    <span class="k">if</span> <span class="n">totalValue</span><span class="p">(</span><span class="n">player_hand</span><span class="p">)</span> <span class="o">==</span> <span class="mi">21</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">totalValue</span><span class="p">(</span><span class="n">dealer_hand</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">21</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1">#player wins after first deal!</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1">#draw</span>

    <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">player_hand</span><span class="p">,</span> <span class="n">dealer_hand</span><span class="p">,</span> <span class="n">status</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">state</span>
</code></pre></div>

<p>There you have it. We've implemented a simplified blackjack game (no double downs or splitting) with just a few functions that basically just consist of some if-else conditions. Here's some sample game-play so you know how to use it.</p>
<div class="highlight"><pre><span></span><code><span class="n">state</span> <span class="o">=</span> <span class="n">initGame</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">((7, False), (5, False), 1)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">state</span> <span class="o">=</span> <span class="n">play</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#Player has total of 7, let&#39;s hit</span>
<span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">((9, False), (5, False), 1)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">state</span> <span class="o">=</span> <span class="n">play</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#player has a total of 9, let&#39;s hit</span>
<span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">((15, False), (5, False), 1)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">state</span> <span class="o">=</span> <span class="n">play</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1">#player has a total of 15, let&#39;s stay</span>
<span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">((15, False), (20, False), 4)</span>
</code></pre></div>

<p>Damn, I lost. Oh well, that should demonstrate how to use the blackjack game. As a user, we only have to concern ourselves with the <code>initGame()</code> and <code>play()</code> functions. <code>initGame()</code> just creates a random state by dealing the player 2 random cards and the dealer one random card and setting the game status to 1 ('in progress'). <code>play()</code> accepts a state and an action (either 0 or 1, for 'stay' and 'hit', respectively). Please keep in mind the distinction between a blackjack game state and the state with respect to our Reinforcement Learning (RL) algorithm. We will compress the states a bit by ignoring the useable ace boolean for the dealer's hand because the dealer only shows a single card and if it's an ace the player has no idea if it's useable or not, so it offers no additional information to us.</p>
<h3>Time for Reinforcement Learning</h3>
<p>Let's start the real fun: building our Monte Carlo-based reinforcement learning algorithm. Here's the algorithm words/math (adapted from the Sutton &amp; Barto text):</p>
<ol>
<li>Choose a random state <span class="math">\(S_0 \in \mathcal{S}\)</span> (some state in the set of all possible states); this is what <code>initGame()</code> does</li>
<li>Take action <span class="math">\(A_0 \in \mathcal{A(S_0)}\)</span> (take some action in set of all possible actions given we're in state <span class="math">\(S_0\)</span>)</li>
<li>Generate a complete episode starting from <span class="math">\(S_0\,\ A_0\)</span> following policy <span class="math">\(\pi\)</span></li>
<li>For each pair <span class="math">\(s, a\)</span> occuring in the episode:<ol>
<li><span class="math">\(G = \text{returns/rewards following the first occurence of s,a}\)</span></li>
<li>If this is the first experience of <span class="math">\(s, a\)</span> in any episode, simply store <span class="math">\(G\)</span> in our <span class="math">\(Q(s, a)\)</span> table. If it's not the first time, then recalculate the average returns and store in <span class="math">\(Q(s, a)\)</span>.</li>
</ol>
</li>
<li>For each state <span class="math">\(s\)</span> in the episode: We use an <span class="math">\(\epsilon\)</span>-greedy action select process such that <span class="math">\(\pi(s) = argmax_a{Q(s, a)}\)</span> most of the time but with probability <span class="math">\(\epsilon\)</span>, <span class="math">\(\pi(s) = random(A_0 \in \mathcal{A(S_0)})\)</span> (basically the same as our n-armed bandit policy function). Recall that we use an epsilon-greedy policy function to ensure we have a good balance of exploration versus exploitation.</li>
</ol>
<p>In essence, with Monte Carlo we are playing randomly initialized games, sampling the state-action pair space and recording returns. In doing so, we can iteratively update our policy <span class="math">\(\pi\)</span>.</p>
<p>Let's get to coding.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">#Create a list of all the possible states</span>
<span class="k">def</span> <span class="nf">initStateSpace</span><span class="p">():</span>
    <span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">card</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span><span class="mi">22</span><span class="p">):</span>
            <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">val</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">card</span><span class="p">))</span>
            <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">val</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">card</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">states</span>

<span class="c1">#Create a dictionary (key-value pairs) of all possible state-actions and their values</span>
<span class="c1">#This creates our Q-value look up table</span>
<span class="k">def</span> <span class="nf">initStateActions</span><span class="p">(</span><span class="n">states</span><span class="p">):</span>
    <span class="n">av</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
        <span class="n">av</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">av</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">return</span> <span class="n">av</span>
<span class="c1">#Setup a dictionary of state-actions to record how many times we&#39;ve experienced</span>
<span class="c1">#a given state-action pair. We need this to re-calculate reward averages</span>
<span class="k">def</span> <span class="nf">initSAcount</span><span class="p">(</span><span class="n">stateActions</span><span class="p">):</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">sa</span> <span class="ow">in</span> <span class="n">stateActions</span><span class="p">:</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">sa</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">counts</span>

<span class="c1">#This calculates the reward of the game, either +1 for winning, 0 for draw, or -1 for losing</span>
<span class="c1">#We can determine this by simply substracting the game status value from 3</span>
<span class="k">def</span> <span class="nf">calcReward</span><span class="p">(</span><span class="n">outcome</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">3</span><span class="o">-</span><span class="n">outcome</span>

<span class="c1">#This recalculates the average rewards for our Q-value look-up table</span>
<span class="k">def</span> <span class="nf">updateQtable</span><span class="p">(</span><span class="n">av_table</span><span class="p">,</span> <span class="n">av_count</span><span class="p">,</span> <span class="n">returns</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">returns</span><span class="p">:</span>
        <span class="n">av_table</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">av_table</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">av_count</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">returns</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">-</span> <span class="n">av_table</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">av_table</span>

<span class="c1">#returns Q-value/avg rewards for each action given a state</span>
<span class="k">def</span> <span class="nf">qsv</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">av_table</span><span class="p">):</span>
    <span class="n">stay</span> <span class="o">=</span> <span class="n">av_table</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span><span class="mi">0</span><span class="p">)]</span>
    <span class="n">hit</span> <span class="o">=</span> <span class="n">av_table</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span><span class="mi">1</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">stay</span><span class="p">,</span> <span class="n">hit</span><span class="p">])</span>

<span class="c1">#converts a game state of the form ((player total, ace), (dealer total, ace), status) </span>
<span class="c1">#to a condensed state we&#39;ll use for our RL algorithm (player total, usable ace, dealer card)</span>
<span class="k">def</span> <span class="nf">getRLstate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">player_hand</span><span class="p">,</span> <span class="n">dealer_hand</span><span class="p">,</span> <span class="n">status</span> <span class="o">=</span> <span class="n">state</span>
    <span class="n">player_val</span><span class="p">,</span> <span class="n">player_ace</span> <span class="o">=</span> <span class="n">player_hand</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">player_val</span><span class="p">,</span> <span class="n">player_ace</span><span class="p">,</span> <span class="n">dealer_hand</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

<p>Above we've defined basically all the functions we need to run our Monte Carlo algorithm. We initialize our state and state-action space, define methods to calculate rewards and update our state-action table (Q-value table). Below is where we'll actually run 5,000,000 Monte Carlo simulations of blackjack and fill out our Q-value table.</p>
<div class="highlight"><pre><span></span><code><span class="n">epochs</span> <span class="o">=</span> <span class="mi">5000000</span> <span class="c1">#takes just a minute or two on my Macbook Air</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">state_space</span> <span class="o">=</span> <span class="n">initStateSpace</span><span class="p">()</span>
<span class="n">av_table</span> <span class="o">=</span> <span class="n">initStateActions</span><span class="p">(</span><span class="n">state_space</span><span class="p">)</span>
<span class="n">av_count</span> <span class="o">=</span> <span class="n">initSAcount</span><span class="p">(</span><span class="n">av_table</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1">#initialize new game; observe current state</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">initGame</span><span class="p">()</span>
    <span class="n">player_hand</span><span class="p">,</span> <span class="n">dealer_hand</span><span class="p">,</span> <span class="n">status</span> <span class="o">=</span> <span class="n">state</span>
    <span class="c1">#if player&#39;s total is less than 11, increase total by adding another card</span>
    <span class="c1">#we do this because whenever the player&#39;s total is less than 11, you always hit no matter what</span>
    <span class="c1">#so we don&#39;t want to waste compute cycles on that subset of the state space</span>
    <span class="k">while</span> <span class="n">player_hand</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">11</span><span class="p">:</span>
        <span class="n">player_hand</span> <span class="o">=</span> <span class="n">add_card</span><span class="p">(</span><span class="n">player_hand</span><span class="p">,</span> <span class="n">randomCard</span><span class="p">())</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">player_hand</span><span class="p">,</span> <span class="n">dealer_hand</span><span class="p">,</span> <span class="n">status</span><span class="p">)</span>
    <span class="n">rl_state</span> <span class="o">=</span> <span class="n">getRLstate</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1">#convert to compressed version of state</span>

    <span class="c1">#setup dictionary to temporarily hold the current episode&#39;s state-actions</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1">#state, action, return</span>
    <span class="k">while</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span> <span class="c1">#while in current episode</span>
        <span class="c1">#epsilon greedy action selection</span>
        <span class="n">act_probs</span> <span class="o">=</span> <span class="n">qsv</span><span class="p">(</span><span class="n">rl_state</span><span class="p">,</span> <span class="n">av_table</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">):</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">act_probs</span><span class="p">)</span><span class="c1">#select an action</span>
        <span class="n">sa</span> <span class="o">=</span> <span class="p">((</span><span class="n">rl_state</span><span class="p">,</span> <span class="n">action</span><span class="p">))</span>
        <span class="n">returns</span><span class="p">[</span><span class="n">sa</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">#add a-v pair to returns list, default value to 0</span>
        <span class="n">av_count</span><span class="p">[</span><span class="n">sa</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1">#increment counter for avg calc</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">play</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span> <span class="c1">#make a play, observe new state</span>
        <span class="n">rl_state</span> <span class="o">=</span> <span class="n">getRLstate</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="c1">#after an episode is complete, assign rewards to all the state-actions that took place in the episode</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">returns</span><span class="p">:</span> 
        <span class="n">returns</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">calcReward</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">av_table</span> <span class="o">=</span> <span class="n">updateQtable</span><span class="p">(</span><span class="n">av_table</span><span class="p">,</span> <span class="n">av_count</span><span class="p">,</span> <span class="n">returns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">Done</span>
</code></pre></div>

<p>Okay, so we just ran a Monte Carlo simulation of blackjack 5,000,000 times and built up an action-value (Q-value) table that we can use to determine what the optimal action is when we're in a particular state.</p>
<p>How do we know if it worked? Well, below I've written some code that will show us a 3d plot of the dealer's card, player's total and the Q-value for that state (limited to when the player does not have a useable ace). You can compare to a very similar plot shown in the Sutton &amp; Barto text on page 117, compare to this (http://waxworksmath.com/Authors/N_Z/Sutton/WWW/Chapter_5/op_bj_results.html)</p>
<div class="highlight"><pre><span></span><code><span class="c1">#3d plot of state-value space where no useable Aces are present</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">,</span> <span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Dealer card&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Player sum&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;State-Value&#39;</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span> <span class="o">=</span> <span class="p">[],[],[]</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_space</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">key</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">11</span> <span class="ow">and</span> <span class="n">key</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">21</span><span class="p">):</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">state_value</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">av_table</span><span class="p">[(</span><span class="n">key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)],</span> <span class="n">av_table</span><span class="p">[(</span><span class="n">key</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]])</span>
        <span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state_value</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">azim</span> <span class="o">=</span> <span class="mi">230</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_trisurf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=.</span><span class="mi">02</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">jet</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="o">&lt;</span><span class="n">mpl_toolkits</span><span class="o">.</span><span class="n">mplot3d</span><span class="o">.</span><span class="n">art3d</span><span class="o">.</span><span class="n">Poly3DCollection</span> <span class="n">at</span> <span class="mh">0x1105d2358</span><span class="o">&gt;</span>
</code></pre></div>

<p><img alt="png" src="http://outlace.com/images/rlpart2/output_17_1.png"></p>
<p>Looks pretty good to me. This isn't a major point, but notice that I plotted the State-Value on the z-axis, not an action value. I calculated the state value by simply taking the largest action value for a state from our state-action lookup table. Thus, the value of a state is equivalent to the average rewards following the best action.</p>
<p>Below I've used our action-value lookup table to build a crappy looking table that displays the optimal actions one should take in a game of blackjack given you're in a particular state. The left column are the possible player totals (given no useable ace) and the top row is the possible dealer cards. So you can lookup what's the best action to take if I have a total of 16 and the dealer is showing a 7 (the answer is "hit"). You can compare to wikipedia's article on blackjack that has a similar table: https://en.wikipedia.org/wiki/Blackjack#Basic_strategy  As you can tell, ours is pretty accurate.</p>
<table><tr><th><td>Dealer</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></th></tr><tr><td>Player</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>12</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td>S</td><td>S</td><td>S</td><td>S</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td></tr><tr><td>13</td><td style='background-color:red;'>H</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td></tr><tr><td>14</td><td style='background-color:red;'>H</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td></tr><tr><td>15</td><td style='background-color:red;'>H</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td></tr><tr><td>16</td><td style='background-color:red;'>H</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td><td style='background-color:red;'>H</td></tr><tr><td>17</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td></tr><tr><td>18</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td></tr><tr><td>19</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td></tr><tr><td>20</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td><td>S</td></tr></table>

<h3>Conclusion &amp; What's Next</h3>
<p>Here we've covered Monte Carlo reinforcement learning methods that depending on stochastically sampling the environment and iteratively improving a policy <span class="math">\(\pi\)</span> after each episode. One disadvantage of Monte Carlo methods is that we must wait until the end of an episode to update our policy. For some types of problems (like blackjack), this is okay, but in a lot of cases, it makes more sense to able to learn at each time step (immediately after each action is taken).</p>
<p>The whole point of the Monte Carlo simulations were to build an action-value table. The action-value table basically <em>is</em> our <span class="math">\(Q(s,a)\)</span> function. You give it a state and an action and it just goes and looks up the value in the table. The most important thing to learn from all of this is that in essentially any RL method, our goal is to find an optimal <span class="math">\(Q\)</span> function. Most of the differences between RL algorithms revolve around differences in determining Q-values. The policy function is straightforward, just pick the best action using <span class="math">\(Q(s,a)\)</span>. We might throw in a softmax or something to add in some randomness, but there's not a lot more to <span class="math">\(\pi(s)\)</span>.</p>
<p>In the next part, I will abandon tabular learning methods and cover Q-learning (a type of temporal difference (TD) algorithm) using a neural network as our <span class="math">\(Q\)</span> function (what we've all been waiting for).</p>
<p>This was a pretty meaty post so please email me (outlacedev@gmail.com) if you spot any errors or have any questions or comments.</p>
<h3>Download this IPython Notebook</h3>
<p>https://github.com/outlace/outlace.github.io/blob/master/ipython-notebooks/rlpart2.ipynb</p>
<h3>References:</h3>
<ol>
<li>https://en.wikipedia.org/wiki/Blackjack</li>
<li>https://en.wikipedia.org/wiki/Monte_Carlo_method</li>
<li>"Reinforcement Learning: An Introduction" Sutton &amp; Barto</li>
<li>https://inst.eecs.berkeley.edu/~cs188/sp08/projects/blackjack/blackjack.py (Adapted some code from here)</li>
<li>http://waxworksmath.com/Authors/N_Z/Sutton/WWW/Chapter_5/op_bj_results.html</li>
</ol>
<div class="highlight"><pre><span></span><code>
</code></pre></div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <div class="clear"></div>

                <div class="info">
                    <a href="http://outlace.com/rlpart2.html">posted at 00:00</a>
                    by Brandon Brown
                    &nbsp;&middot;&nbsp;<a href="http://outlace.com/category/reinforcement-learning/" rel="tag">Reinforcement-Learning</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://outlace.com/tag/monte-carlo/" class="tags">Monte-Carlo</a>
                    &nbsp;<a href="http://outlace.com/tag/rl/" class="tags">RL</a>
                </div>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'outlace';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
            </article>
            <div class="clear"></div>
            <footer>
                <p>
                <!--- <a href="http://outlace.com/feeds/all.atom.xml" rel="alternate">Atom Feed</a> --->
                <a href="mailto:outlacedev@gmail.com"><i class="svg-icon email"></i></a>
                <a href="http://github.com/outlace"><i class="svg-icon github"></i></a>
                <a href="http://outlace.com/feeds/all.atom.xml"><i class="svg-icon rss"></i></a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-65814776-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
</body>
</html>