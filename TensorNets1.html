<!DOCTYPE html>
<html lang="en">
<head>
    
        <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Δ ℚuantitative √ourney | Tensor Networks</title>
    <link rel="shortcut icon" type="image/png" href="http://outlace.com/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="http://outlace.com/favicon.ico">
    <link href="http://outlace.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Δ ℚuantitative √ourney Full Atom Feed" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="http://outlace.com/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Brandon Brown" />

    <meta name="keywords" content="Tensor-Networks" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="http://outlace.com/">Home</a></li>
                <li><a href="http://outlace.com/pages/about.html">About</a></li>
                <li><a href="http://outlace.com/tags/">Tags</a></li>
                <li><a href="http://outlace.com/categories/">Categories</a></li>
                <li><a href="http://outlace.com/archives/{slug}/">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="http://outlace.com/">Δ ℚuantitative √ourney</a></h1>
            <h2>Science, Math, Statistics, Machine Learning ...</h2>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Jun 19, 2018</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://outlace.com/TensorNets1.html" rel="bookmark" title="Permanent Link to &quot;Tensor Networks&quot;">Tensor Networks</a>
                </h2>



                <h1>On Deep Tensor Networks and the Nature of Non-Linearity</h1>
<p><strong>Abstract</strong>: Tensor Networks can be seen as a higher-order generalization of traditional deep neural networks, and yet they lack an explicit non-linearity such as applying the ReLU or sigmoid function as we do with neural nets. In this article, we cover a different perspective on what linearity vs nonlinearity means by treating function arguments as "resources" that can be consumed. Linear functions are those that "conserve resources" (i.e. they don't copy or delete their inputs). </p>
<p>We then cover what tensor networks are, how they work, and how this different notion of linearity gives insight into how tensor networks can learn non-linear transformations. We demonstrate that while tensor networks are locally linear, they can exhibit global non-linear behavior due to the action of copying input data and inpendently transforming each copy. Hence, unlike neural networks, the non-linearity of tensor networks arises soley from the architecture and topology of the network itself. Lastly, we demonstrate how a simple tensor network can perform reasonably well on the FashionMNIST dataset using PyTorch's <code>einsum</code> function.</p>
<p><img src="images/TensorNetwork/Tensor_Network.png" width=400px></p>
<p>Deep learning algorithms (neural networks) in their simplest form are
just a sequence of two operations composed to some depth: a linear transformation (i.e. a matrix-vector multiplication) followed by the element-wise application of a reasonably well-behaved non-linear function (called the activation function). Together the linear transformation and the non-linear function are called a "layer" of the network, and the composition of many layers forms a deep neural network. Both the depth and the non-linearity of deep neural networks are crucial for their generalizability and learning power.</p>
<p>The most credible explanation for why depth matters in neural networks is that depth models hierarchy, i.e. the idea that some data are at a higher and lower levels of abstraction. "My Jack Terrier dog Spot" is a concrete instance of the more abstract concept of "dog" or even "animal." Hence depth naturally models these kinds of hierarchical relationships in data, and it turns out pretty much all of the data we care about has a hierarchical structure (e.g. letters -&gt; words -&gt; sentences, or musical notes -&gt; chords -&gt; bars -&gt; songs). Another word for hierarchy is composition; complex things are often composed of simpler things, which are composed of yet simpler things. Hence <em>deep</em> neural networks have an (inductive) bias toward learning these compositional/hierarchical relationships, which is exactly what we want them to do.</p>
<p>Okay so depth is important. But why is the non-linear "activation" function so important for deep learning? 
An almost tautological answer is that if we want our neural network to be able to model non-linear relationships, it must have some non-linearity itself.</p>
<p>Another reason is that without the non-linear functions, a sequence of just linear transformation layers
can always be "collapsed" into a single linear transformation. This is basic linear algebra; the product
of any sequence of matrices (i.e. matrix composition, hence composition of linear transformations)
can always be reduced to a single matrix/linear transformation. So without the non-linear function in each layer
we forfeit the compositionality (depth) property of deep neural networks, which allow hierarchical abstraction as 
you go deeper in the network.</p>
<p>Since we generally use "bias units" in the linear portion of a layer, every layer can actually perform
affine transformations (linear transformation + a translation). We know from linear algebra that
affine transformations can only uniformly stretch and shrink (scale), rotate, sheer and translate vectors in a vector space. If we think of some data of interest as a point cloud in some N-dimensional space, then if it is non-random data, it will have some sort of shape.</p>
<p>For example, some fairly non-complex periodic system might produce data points that lie on a 2D circle or a higher-dimensional loop. The typical goal for a supervised neural network is to map this point cloud onto some other point cloud living in a different space with a different shape. When we one-hot encode the output of a neural network then we're mapping our point cloud onto essentially orthonormal unit basis vectors in a different space. Quite literally, we're transforming some geometric shape (or space) into a different shape. The task of the neural network then is to figure how to construct a new shape (the output/target space) from a starting shape (the input data point cloud) using only the tools of affine transformations and (usually) a single type of non-linear function, such as the rectified linear unit (ReLU) or sigmoid.</p>
<div style="display:table">
    <div style="display: table-row;">
    <div style="float:left;display: table-cell; margin-right:50px; "><img src="images/TensorNetwork/sigmoid_activation.png" width=350px></div>
    <div style="display: table-cell; "><img src="images/TensorNetwork/relu_activation.png" width=350px></div>
   </div>
</div>

<p>On the left is a graph of the sigmoid activation function. Clearly it is a curved function and hence not linear. Sigmoid has mostly fallen out of favor and ReLU has become the de facto standard activation function. The right figure is the graph for ReLU. ReLU is just <span class="math">\(max(0,x)\)</span>, and has two "pieces," the flat line for when <span class="math">\(x \lt 0\)</span> and the sloped line for when <span class="math">\(x \geq 0\)</span>. So both of ReLU's parts are lines, why does it count as a non-linear function? A standard, but unintuitive, mathematical definition for a linear function is a function that has the following properties:</p>
<p>(Definition 1.1: Linear Function)
</p>
<div class="math">$$
f : X \rightarrow Y  \\ 
\text{$f$ is a function from some domain X to some codomain Y} \\ 
f(x_1 + x_2) = f(x_1) + f(x_2) \\
f(a \times x) = a \times f(x)
$$</div>
<p>If you check to see if ReLU has these properties you'll find that it fails both properties, and hence it is not a linear function. For example, <span class="math">\(relu(-1+1) \neq relu(-1) + relu(1)\)</span> and <span class="math">\(relu(-2 * 5) \neq -2*relu(5)\)</span>.</p>
<p>But this "linearity checklist" lacks a meaningful interpretation. I can use it to identify functions as linear or not
but it doesn't give me any intuition.</p>
<p>Here's a better (more intuitive after some explanation) definition of a linear function. If we treat the input arguments of a function as resources, then:</p>
<p>(Definition 1.2: Linear Function)
- A linear function is a function whose input data are always used exactly <em>once</em>. No more, no less.</p>
<p>This is a better definition from an intuitive point of view, but of course it lacks the mathematical precision of the linearity checklist. Nonetheless, the idea is that if we treat data as a resource that can be consumed
and transformed then a linear function is a function that completely consumes its input data without copying (i.e. duplicating) or discarding it. This notion can be formalized, but we'll wait for later.</p>
<p>A linear function is like a chemical reaction. If you mix oxygen molecules with hydrogen molecules and
add some energy you'll cause them to react to produce water molecules. The reagents are completely used up to produce the water molecule products. It is impossible to clone the oxygen molecules and it's impossible to delete them; all you can do is react them with other molecules. Indeed, the term "linear" as describing a function obviously has its roots in the easily visualizable line, but with definition 1.2, perhaps a better term might be "reactive function".</p>
<p>Consider this simple function: <span class="math">\(f(x) = mx\)</span>, where <span class="math">\(m\)</span> is some constant. It is immediately clear that <span class="math">\(x\)</span> only appears once on the right hand side of the equation, thus it is only "used" once. In contrast the non-linear function <span class="math">\(f(x) = x^2\)</span> has <span class="math">\(x\)</span> appearing twice on the right hand side of the equation (since <span class="math">\(x^2 = x \times x\)</span>) and it is irreducible. The input argument is being implicitly cloned or copied in the function. It's as if there is a hidden sub-function <span class="math">\(\delta: x \rightarrow (x, x)\)</span>, such that we can rewrite <span class="math">\(f(x) = x^2\)</span> as <span class="math">\(f(x) = *\delta(x)\)</span>, where the multiplication operator is acting as a prefix notated operator over the 2-tuple that the <span class="math">\(\delta\)</span> function produces. This looks nicer in code:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">clone</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">x</span>
<span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">clone</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">square</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="mf">25</span>
</code></pre></div>

<p>Our slightly more mathematically precise definition of a non-linear function is thus most easily phrased in the language of computation.</p>
<p>(Definition 1.3: Non-linear Function OR the <strong>No-Cloning Rule</strong>)
- A non-linear function is any function that at any point in its computation copies its input data. A linear function never duplicates its input at any point it the computation of its result.</p>
<p>Note that although we can make a function such as <span class="math">\(f(x) = x + x\)</span>, which appears to similarly clone its input, we can always reduce that expression to a non-cloned version, namely <span class="math">\(f(x) = 2x\)</span>, hence it is linear. With <span class="math">\(f(x) = x^2\)</span> there is no way to reduce that expression to one where <span class="math">\(x\)</span> only appears once on the right hand side of the equation. In other words, if you were to write a computer program to calculate or even hand calculate <span class="math">\(f(x) = x^2\)</span> you would
necessarily need to work with 2 copies of the value of <span class="math">\(x\)</span>, whereas with <span class="math">\(f(x) = 2x\)</span> you could evaluate the result with just a single copy. This easily extends to multivariate functions, such as <span class="math">\(g(x,y) = x + y\)</span>. In this case we would say <span class="math">\(g(x,y)\)</span> is linear with respect to each of its input arguments, or <strong>multilinear</strong>. Individually, <span class="math">\(x\)</span> and <span class="math">\(y\)</span> respect the linearity checklist and the no-cloning rule.</p>
<p>The no-cloning rule for non-linearity is reminiscent of the law of conservation of energy from physics (and the no-cloning theorem of quantum mechanics); indeed it can be construed as the information theory equivalent: "information is neither created nor destroyed in an isolated process (function)". This analogy will prove useful later.</p>
<p>With definitions 1.2 and 1.3 we now have a meaningful explanation for why the ReLU activation function is non-linear, it's because it uses the non-linear mechanism of copying. It violates the no-cloning rule. How? Well because it involves a conditional. Remember, <span class="math">\(relu(x) = max(0,x)\)</span> and 
</p>
<div class="math">$$
\begin{align}
max(0,x) &amp; = 0, &amp; x &lt; 0, \\
 &amp; = x, &amp; x &gt;= 0
\end{align}
$$</div>
<p>In order to evaluate a conditional based on some input, you have to first duplicate the input so you can use one copy to evaluate (or measure) it and then use the other copy to actually do what you want with.</p>
<p>Okay, so we have a good intuition for what linear versus non-linear means in terms of the copiability of resources, but how is that intuition useful? Well my eventual goal here is to explore how definitions 1.2 and 2.3 explain how tensor networks work.</p>
<p>One interesting path is information theory. If you have a non-linear function like <span class="math">\(f(x) = x^2\)</span>, it is non-linear because <span class="math">\(x\)</span> is copied. However, consider a function like <span class="math">\(f(x,y) = x*y\)</span>, it is called multi-linear because it is linear with respect to each of its input variables. Interestingly, if <span class="math">\(y = x\)</span> (always) then this function behaves exactly like f(x)=x^2 but it's just in a different form. More generally, if <span class="math">\(f(x,y) = x*y\)</span> but y is merely correlated with x, say <span class="math">\(y = x + e\)</span> (e = noise) then you'll get f(x,y) = x(x+e) = x^2 + ex, which for a sufficiently small <span class="math">\(e\)</span> will behave nearly the same as f(x) = x^2. So just a linear function of two variables that are highly correlated will produce non-linear behavior. Hence, if you have say two separate_linear_ dynamic systems that start off completely un-entangeled with each having their own maximum degrees of freedom, if they start to become correlated/entangled with each other by interaction, then when taken together as a composite system, it may exhibit non-linear dynamics.</p>
<h3>A brief detour into calculus</h3>
<p>Let's take a bit of a detour into the world of calculus. You know that the derivative of a function <span class="math">\(f(x)\)</span> at a point <span class="math">\(a\)</span> is the slope of the tangent line at that point. More generally for multivariate and vector-valued functions, the derivative is thought of as the best linear approximation to that function. You've seen the definition of the derivative in terms of limits or in non-standard analysis as an algebraic operation with an extended real number system (the hyperreals).</p>
<p>You know how to differentiate simple functions such as <span class="math">\(f(x) = x^3\)</span> by following some derivative rules. For example, in this case we apply the rule: "take the exponent of <span class="math">\(x\)</span> and multiply it by <span class="math">\(x\)</span> and then reduce the power of <span class="math">\(x\)</span> by 1." So we calculate <span class="math">\(\frac{df}{dx} = 3x^2\)</span> (where <span class="math">\(\frac{df}{dx}\)</span> is Leibniz notation, read as "the derivative of the function <span class="math">\(f\)</span> with respect to variable <span class="math">\(x\)</span>"). In a mathematically precise way we declare that if <span class="math">\(f(x)=x^{r}\)</span>, then <span class="math">\(df/dx = rx^{r-1}\)</span>. And of course we've <em>memorized</em> a bunch of other simple rules, which together form a powerful toolset we can use to compute the derivative of many functions.</p>
<p>Since differentiation is all about linear approximations, what new insights might definitions 1.2 and 1.3
give us about this fundamental concept in mathematics? Well, the limit definition of a derivative and all those differentiation rules can all be seen as special cases of applying definition 1.3 to <em>non-linear</em> functions. That is, what would happen if we took a non-linear function and re-construed it as a linear function?</p>
<p>Take our simple function <span class="math">\(f(x) = x^3\)</span> and expand it to make explicit the copying that is happening, <span class="math">\(f(x) = x \cdot x \cdot x\)</span>. This function makes 3 copies of <span class="math">\(x\)</span> and then reacts them together (multiplication) to produce the final result. The copying operation is totally hidden with traditional function notation, but it happened. Where is the copy operation in traditional set-theoretic mathematics? Why is it always hidden in the notation of non-linear functions?</p>
<p>The answer requires back-tracking to linear algebra. Remember how a matrix encodes a linear transformation? A full-rank square matrix will always result in a dimension-preserving transformation, e.g. a full-rank 2x2 matrix will always send a 2D vector to another 2D vector. A non-square matrix such as a 2x3 matrix, on the other hand, will send a 2D vector into a 3-dimensional space.</p>
<p>Thus the copy operation for scalars is the linear algebraic operation of projecting a lower-dimensional vector (even a scalar) into a higher-dimensional space. Thus we can think of <span class="math">\(f(x) = x^3\)</span> as a function that takes a scalar, copies its input 3 times, then multiplies them together.</p>
<div class="highlight"><pre><span></span><code><span class="n">clone</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">clone</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">clone</span><span class="p">))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="k">[5 5 5]</span>
<span class="na">125</span>
</code></pre></div>

<div class="math">$$ f(x) = \prod Ax = x^3$$</div>
<p>Where <span class="math">\(A\)</span> is the "copy" matrix <span class="math">\(<div class="math">\begin{bmatrix} 1 &amp; 1 &amp; 1 \end{bmatrix}</div>\)</span>.</p>
<p>What does this have to do with calculus? Well let's convert <span class="math">\(f(x)=x\cdot x \cdot x\)</span> into a multi-linear function, i.e. <span class="math">\(f(x,y,z) = x\cdot y \cdot z\)</span>, except that we know <span class="math">\(x = y = z\)</span> (or that they're highly correlated). It is multi-linear because if we assumed data independence, then the function is linear with respect to each variable. For example, <span class="math">\(f(5+3,y,z) = 8yz\)</span> which is the same as <span class="math">\(f(5,y,z) + f(3,y,z) = 5yz + 3yz = 8yz\)</span>. But since we know our data is not independent, technically this function <em>isn't</em> linear because we can't independently control for each variable. But let's for a momement pretend we didn't know our input data is correlated in this way.</p>
<p>So remember, we already remember from school that the derivative of <span class="math">\(f(x)=x^3\)</span> is <span class="math">\(\frac{df}{dx} = 3x^2\)</span>. We can't take "the derivative" of our new multi-linear function anymore, we can only take partial derivatives with respect to each variable. To take the partial derivative of <span class="math">\(x\)</span>, we assume the other variables <span class="math">\(y,z\)</span> are held as constants, and then it becomes a simple linear equation. We do the same for <span class="math">\(y,z\)</span>.</p>
<div class="math">$$ 
\begin{align}
\frac{\partial{f}}{\partial{x}} = yz &amp;&amp; \frac{\partial{f}}{\partial{y}} = xz &amp;&amp; \frac{\partial{f}}{\partial{z}} = yx 
\end{align}
$$</div>
<p>But wait, we can't really hold the other variables constant because we know they're perfectly correlated (actually equal). What we're getting at is a case of the total derivative (see &lt; https://en.wikipedia.org/wiki/Total_derivative &gt;), which is how you take the derivative of a multivariable function with interacting variables. In this particular case, since <span class="math">\(x = y = z\)</span>, we just need to combine (sum) all these partial derivatives together and make all the variables the same, say <span class="math">\(x\)</span>.</p>
<div class="math">$$ \frac{df}{dx} = yz + xz + yx = xx + xx + xx = 3x^2 $$</div>
<p>Exactly what we get with through the traditional route. Or consider a slightly more complicated function:
</p>
<div class="math">$$
f(x) = 3x^3+x^2 \rightarrow \frac{df}{dx} = 9x^2 + 2x \\
f(a,b,c,d,e) = 3abc+de \rightarrow \frac{df}{d\{abcde\}} = 3bc+3ac+3ab + e + d \rightarrow 9x^2 + 2x
$$</div>
<p>Hence an intuition for copying as non-linearity even makes computing derivatives more intuitive, at least for me.</p>
<h3>Tensor Networks</h3>
<p>In the tensor algebra, a scalar is a 0-tensor, a vector is a 1-tensor, a matrix is a 2-tensor, and higher order tensors don't generally have names. But in order for the linear algebra operation we just did, we had to promote a 0-tensor to a 1-tensor. Tensors are often notated by labeled indices, as if they were containers and we find elements of the container by an addressing mechanism. A 0-tensor (scalar) isn't a container, it is the "atomic ingredient" of higher tensors, hence it does not have indices. Once you box together a bunch of scalars, you get a 1-tensor (a vector), and now to locate the individual scalars in the box, we label each one with a positive integer. </p>
<p>If we have a vector <span class="math">\(A = \langle{a,b,c}\rangle\)</span>, where <span class="math">\(a,b,c\)</span> are scalars, then we could label them <span class="math">\(1,2,3\)</span> in order. Hence we could refer to the <span class="math">\(i'th\)</span> element of <span class="math">\(A\)</span> as <span class="math">\(A_i\)</span> or <span class="math">\(A(i)\)</span>. So <span class="math">\(A_1 = a\)</span> or <span class="math">\(A(3) = c\)</span>. Now we can box together a bunch of 1-tensors (vectors) and we'll get a 2-tensor (matrix). A matrix <span class="math">\(M(i,j)\)</span> hence would have two indices, so we need to supply two numbers to find a single scalar in the matrix. If we supply a partial address, such as <span class="math">\(M(1,j)\)</span> then this would return a 1-tensor, whereas <span class="math">\(M(1,3)\)</span> would return a 0-tensor. We can box together a bunch of 2-tensors to get a 3-tensor and so on. Importantly, anytime you box some <span class="math">\(k\)</span>-tensors together to form a higher order tensor, they must be of the same size.</p>
<p>Tensors are in a sense compositional mathematical objects. Scalars are "made of" nothing. Vectors are "made of" scalars, matrices are made of vectors, 3-tensors are made of 2-tensors. Perhaps this suggests that tensors have even more power to represent compositionality in data than do conventional neural networks, which usually only represent depth-wise hierarchy.</p>
<p>If we have the natural compositionality of individual tensors, we can network them together to form (deep) tensor networks! As we'll soon see, however, there is no explicit non-linear activation function application in a tensor network, everything appears perfectly linear. Yet we Networking in a neural network is nothing more impressive than matrix multiplication. Tensors have a generalization of matrix multiplication called <strong>tensor contraction</strong>.</p>
<h4>Tensor Contraction</h4>
<p>Take a 2-tensor (matrix) and multiply it with a vector (1-tensor).</p>
<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>array([-2,  5])
</code></pre></div>

<p>The matrix on the left is the identity matrix, so it didn't do anything to our vector, but what's the computation happening and how can it be generalized to tensors?</p>
<p>The answer is tensor (or index) contraction. We'll denote a matrix <span class="math">\(A_{rt}\)</span> and a vector <span class="math">\(B_t\)</span>. Multiplying them together is a simple operation really.</p>
<div class="math">$$ C_r = \sum_{t}A_{rt}\cdot B_t $$</div>
<p>This equation is saying that a tensor contraction between matrix matrix <span class="math">\(A_{rt}\)</span> and a vector <span class="math">\(B_t\)</span> results in a new vector <span class="math">\(C_r\)</span> (because it only has one index), and each element of <span class="math">\(C_r\)</span> indexed by <span class="math">\(r\)</span> is determined by taking the sum of the product of each subset of <span class="math">\(A_{rt}\)</span> with <span class="math">\(B_t\)</span> for every value of <span class="math">\(t\)</span>. Often times the summation symbol is ommitted, so we express a tensor contraction just by juxtaposition, <span class="math">\(A_{rt}B_{t}\)</span>. This often goes by the name <strong>Einstein summation notation</strong>, so whenever you juxtapose two tensors that have at least one shared index it denotes a tensor contraction.</p>
<p>A simpler example is easier to calculate. The inner product of two vectors returns a scalar.</p>
<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="mf">10</span>
</code></pre></div>

<p>This result is from <span class="math">\(1*3+2*2+3*1\)</span>. Let's see why using a tensor contraction. We'll define two vectors <span class="math">\(F_h = \langle{1,2,3}\rangle, G_i = \langle{3,2,1}\rangle $. We can take the inner product of two vectors when they're of equal length, in which case they are, so we can change the index label of $F_h\)</span> to <span class="math">\(F_i\)</span>. Now they have the same indices, we can do the tensor summation.</p>
<div class="math">$$ H = \sum_i F_i\cdot G_i $$</div>
<p>It should be clear that we just multiply together corresponding elements then sum them all up, getting the inner product that numpy computed for us. Note that once we've done the sum over products, the matching indices have been fully contracted and the resulting tensor will thus have 2 less indices than the combined tensor. That is, combining the two vectors we get a tensor with two indices (a 2-tensor), but once we contract them, we get a 0-tensor.</p>
<p>There is a convenient graphical representation of tensors that is really helpful in reasoning about tensor contractions and tensor networks. All you do is represent a tensor as some simple geometric shape, we'll use a square, and then for each index the tensor has, you draw that many number of "legs" or <strong>strings</strong> emanating from the box. For example, here is a matrix and a 3-tensor with 2 and 3 strings, respectively:</p>
<div style="display:table">
    <div style="display: table-row;">
    <div style="float:left;display: table-cell;"><img src="images/TensorNetwork/matrix1.png" width=200px></div>
    <div style="display: table-cell;"><img src="images/TensorNetwork/3tensor1.png" width=200px></div>
   </div>
</div>

<p>Each index has been labeled. Now we can form tensor networks by connecting these tensors if they have compatible indices. For example, here is that inner product we just worked out:</p>
<p><img src="images/TensorNetwork/inner_product1.png"></p>
<p>And here is a matrix vector multiplication:</p>
<p><img src="images/TensorNetwork/matrix_vector_mult.png" width=300px>
You can see the vector on the left since it only has one string exiting from it whereas the matrix on the right has two strings. Once they share a string (an index), that represents a contraction. Notice that the number of remaining open-ended strings represents the rank or order of the tensor that results after the contraction. The matrix-vector multiplication diagram has 1 open string, so we know this contraction returns a new vector.</p>
<p>Since a simple feedforward neural network is nothing more than a series of matrix multiplications with non-linear activation functions, we can represent this easily diagrammatically: </p>
<p><img src="images/TensorNetwork/nn3.png" width=370px></p>
<p>Where the <span class="math">\(\sigma\)</span> (sigma) symbol represents the non-linear function. If we removed that, we would have a tensor network. But remember, this whole post is about non-linearity and tensor networks, so how do we get back the non-linearity in a tensor network?</p>
<p>Copying.</p>
<p>All we need to do is violate the no-cloning rule in the topology of the tensor network, and it will be able to learn non-linear functions. Consider the following two tensor networks. One has two component tensors that are both 3-tensors. Since there are 2 open strings, we know the result is a 2-tensor, however, you can also think of this as a directional network in which we plug in an input vector (or matrix if it's a minibatch) on the left and the network produces an output vector on the right.
<br /><br /></p>
<div style="display:table">
    <div style="display: table-row;">
    <div style="float:left;display: table-cell; margin-right:50px; margin-bottom:25px;"><img src="images/TensorNetwork/tensornet1.png" width=350px></div>
    <div style="display: table-cell; "><img src="images/TensorNetwork/tensornet2.png" width=350px></div>
   </div>
</div>

<p>The tensor network on the left has a tensor <span class="math">\(A\)</span> can can produce two strings from 1 input string, so it has the ability to copying its input, however, both copies get passed to a single tensor <span class="math">\(B\)</span>, so this network cannot produce non-linear behavior because both copies will be entangled and cannot be transformed independently by tensor <span class="math">\(B\)</span>. In contrast, the tensor network on the right <em>can</em> produce non-linear behavior (as we'll soon show) and that's because tensor <span class="math">\(A\)</span> can produce two copies of its input and each copy gets independently transformed by two different tensors <span class="math">\(B,C\)</span>, which then pass their result to tensor <span class="math">\(D\)</span> which computes the final result.</p>
<p>Ready to see some non-linearity arise from what appears to be a purely linear network? Let's see if we can train the tensor network on the right to learn the ReLU non-linear activation function. That would surely be a sign it can do something non-linear. It turns out numpy, PyTorch and TensorFlow all have functions called <strong>einsum</strong> that can compute tensor contractions as we've discussed. Let's see.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</code></pre></div>

<p>Here we define the tensor network by first setting up our individual tensor components. Then we define a function that will accept some input tensor (in this case a scalar) and will connect up the tensors into a network by defining the tensor contractions that will happen.</p>
<p>The notation used for the <em>einsum</em> function in PyTorch, you write the indices of the involved tensors in a string and then pass the actual tensor objects as the second argument in a tuple. In the string, all the tensor indices for each involved tensor should be listed together (as single characters), then a comma for the next tensor with all of its indices and so on. Then you write '-&gt;' to indicate the resultant tensor with its indices. </p>
<p>Take the first string we see, 'sa,abc-&gt;sbc'. This means we will contract two tensors, the first has two indices, the second has three. We label the indices in the way we want the indices to contract. In this case, the 's' index represents the batch size and the 'a' index is the actual data. So we want to contract the data with the 2nd tensor, so we label its first index as 'a' as well. The resulting tensor indices will be whatever indices were not contracted.</p>
<div class="highlight"><pre><span></span><code><span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tensorNet1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sa,abc-&gt;sbc&#39;</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">b1</span><span class="p">))</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sbc,bd-&gt;sd&#39;</span><span class="p">,(</span><span class="n">r1</span><span class="p">,</span><span class="n">b2</span><span class="p">))</span>
    <span class="n">r3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sbc,bd-&gt;sd&#39;</span><span class="p">,(</span><span class="n">r1</span><span class="p">,</span><span class="n">b3</span><span class="p">))</span>
    <span class="n">r4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sd,sd,df-&gt;sf&#39;</span><span class="p">,(</span><span class="n">r2</span><span class="p">,</span><span class="n">r3</span><span class="p">,</span><span class="n">b4</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">r4</span>
</code></pre></div>

<p>If you're familiar with PyTorch, this is just a simple training loop. Normalizing the data seems to be important empirically for tensor contractions. Each tensor in the tensor network is a "trainable object." So if our tensor network contains a 3-tensor with indices of size <span class="math">\(10\times 5\times 10\)</span> then this tensor has <span class="math">\(10 * 5 * 10 = 500\)</span> total number of parameters. The network we've just defined above has a total of <span class="math">\(1*10*10 + 10*10 + 10*10 + 10*1 = 3,000\)</span> parameters.</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">b1</span><span class="p">,</span><span class="n">b2</span><span class="p">,</span><span class="n">b3</span><span class="p">,</span><span class="n">b4</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5000</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tensorNet1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch: </span><span class="si">{}</span><span class="s2"> | Loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">Epoch</span><span class="o">:</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">286692.1875</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">500</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">949.7288818359375</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">1000</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">0.07024684548377991</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">1500</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">0.025008967146277428</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">2000</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">0.02917313575744629</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">2500</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">0.019280623644590378</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">3000</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">0.03735805302858353</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">3500</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">0.039074432104825974</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">4000</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">0.0267774797976017</span>
<span class="n">Epoch</span><span class="o">:</span> <span class="mi">4500</span> <span class="o">|</span> <span class="n">Loss</span><span class="o">:</span> <span class="mf">0.026810143142938614</span>
</code></pre></div>

<p>The loss went steadily down, but let's see if it reliably learns the ReLU function. Remember, we're expecting that it will set negative numbers to 0 (or close to 0) and leave positive numbers as their original value (or at least close to their original values).</p>
<div class="highlight"><pre><span></span><code><span class="n">t1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#Some test data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t1</span><span class="p">)</span>
<span class="n">tensorNet1</span><span class="p">(</span><span class="n">t1</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>tensor([[-0.3854],
        [-0.6216],
        [ 0.2522],
        [-0.6337]])





tensor([[ 0.1582],
        [ 0.4116],
        [ 0.0678],
        [ 0.4277]])
</code></pre></div>

<p>It worked! Well approximately. It's not exactly computing ReLU, but close enough, it's more like it learned how to square the numbers. In any case, the important thing is that it is definitely performing a non-linear function. There is no linear function that can only flip the sign of negative numbers and leave the sign of positive numbers alone. And if you tried this same training task using the "<em>linear</em> topology" from the diagram on the right above, you would be completely unable to learn this non-linear function no matter how you tune the hyperparameters and no matter how big the tensors are. The wiring is everything. The non-linearity arises from the network structure (global) not the individual tensor contractions (locally linear).</p>
<p>To me, this is quite remarkable. That non-linearity can emerge without any explicit non-linear function. Yet it is not that surprising once you understand that no-cloning perspective of linearity. With the right wiring together of tensors, we can allow the tensor computations to return copies of input data, transform each copy independently, then pool together the result. That's how we get non-linearity. Indeed, it has been shown that certain kinds of tensor networks are directly related to convolutional neural networks with max pooling, see &lt; https://arxiv.org/abs/1603.00162 &gt;</p>
<p>The fact that the nonlinearity is due to the network itself suggests that you can dynamically tune how nonlinear the network is, by controlling the degree of data copying that the network can do. I think the generality and flexibility of tensor networks can allow you to design networks with just the right amount of inductive bias given your data. You can design tree networks, grid networks, or any topology you can think of as long as the contractions are possible.</p>
<p><img src="images/TensorNetwork/tensor_topologies.png" /></p>
<h3>Are Tensor Networks useful?</h3>
<p>Okay, so that's all theoretically very interesting, but are tensor networks useful? Are they better than neural networks at anything? Well, tensor networks arose in the Physics community as a powerful tool to simulate quantum systems, so they're definitely useful in that domain. Unfortunately the jury is still out on whether they're "better" than neural networks. Is there an advantage to imposing non-linearity from network topology alone, or is it more efficient to just include an activation function? </p>
<p>So far, I can't tell either from my own experiments with tensor networks or from the literature. However, because the components of tensor networks, tensors, are just high-order structured data, they have shown to be much more interpretable as parameters than the weights of a neural network that get rammed through activation functions. One small benefit I've noticed (but take it with a big grain of salt) is that I can use a much higher learning rate with a tensor network than a neural network without causing divergence during training.</p>
<p>Tensor networks can be very useful in a method called <strong>tensor decompositions</strong>. The idea is that if you have a huge matrix of data, say 1,000,000 x 1,000,000 entries, you can decompose it into a the contraction of a series of smaller tensors, such that when contracted they will sufficiently approximate the original matrix. It turns out that when you train a tensor decomposition network to approximate your data, it will often learn rather interpretable features.</p>
<p><img src="images/TensorNetwork/tensornet5.png" /></p>
<p>Below, I've included some code for a linear (i.e. it cannot learn a non-linear function) tensor network that can be trained to classify FashionMNIST clothes. A 2-layer convolutional neural network with about 500,000 parameters can achieve over 92% accuracy, whereas this simple linear tensor network can achieve about 88.5% accuracy (but quite quickly). The reason for using a linear tensor network is mostly because a decently sized tensor network with non-linear topology (e.g. a hierarchical tree like the figure in the beginning of the post) would be too much code, and a bit too confusing to read with raw einsum notation. </p>
<p>I do include a slightly more complex non-linear topology at the very end that does achieve up to 94% train/ 90% test accuracy, which is getting competitive conventional neural networks. Whether more complex topologies of tensor networks can achieve even better results is left as an exercise for the reader.</p>
<h3>Training a (Linear) Tensor Network on Fashion MNIST</h3>
<h4>Setup a training function</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_accu</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># trainning</span>
        <span class="n">ave_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">correct_cnt</span><span class="p">,</span> <span class="n">ave_loss</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>
        <span class="n">total_cnt</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">*</span><span class="n">shape</span><span class="p">))</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">ave_loss</span> <span class="o">=</span> <span class="n">ave_loss</span> <span class="o">*</span> <span class="mf">0.9</span> <span class="o">+</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.1</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">pred_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total_cnt</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">correct_cnt</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">((</span><span class="n">pred_label</span> <span class="o">==</span> <span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
            <span class="n">acc</span> <span class="o">=</span> <span class="n">correct_cnt</span> <span class="o">/</span> <span class="n">total_cnt</span>
            <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">train_accu</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="sd">&#39;&#39;&#39;if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):</span>
<span class="sd">                print(&#39;==&gt;&gt;&gt; epoch: {}, batch index: {}, train loss: {:.6f}, accuracy: {}&#39;.format(</span>
<span class="sd">                    epoch, batch_idx+1, ave_loss, acc))&#39;&#39;&#39;</span>
        <span class="c1"># testing</span>
        <span class="n">correct_cnt</span><span class="p">,</span> <span class="n">ave_loss</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>
        <span class="n">total_cnt</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">*</span><span class="n">shape</span><span class="p">))</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">pred_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total_cnt</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">correct_cnt</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">((</span><span class="n">pred_label</span> <span class="o">==</span> <span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
            <span class="c1"># smooth average</span>
            <span class="n">ave_loss</span> <span class="o">=</span> <span class="n">ave_loss</span> <span class="o">*</span> <span class="mf">0.9</span> <span class="o">+</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.1</span>
            <span class="n">acc</span> <span class="o">=</span> <span class="n">correct_cnt</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">total_cnt</span>
            <span class="k">if</span><span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;==&gt;&gt;&gt; epoch: </span><span class="si">{}</span><span class="s1">, batch index: </span><span class="si">{}</span><span class="s1">, test loss: </span><span class="si">{:.6f}</span><span class="s1">, acc: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span> \
                      <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ave_loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_accu</span>
</code></pre></div>

<h4>Load up the FashionMNIST Data</h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">dset</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>

<span class="n">train_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s2">&quot;fashion_mnist&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s2">&quot;fashion_mnist&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>


<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
                 <span class="n">dataset</span><span class="o">=</span><span class="n">train_set</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">drop_last</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
                <span class="n">dataset</span><span class="o">=</span><span class="n">test_set</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;==&gt;&gt;&gt; total trainning batch number: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;==&gt;&gt;&gt; total testing batch number: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>==&gt;&gt;&gt; total trainning batch number: 600
==&gt;&gt;&gt; total testing batch number: 100
</code></pre></div>

<h4>Define the Tensor Network</h4>
<h5>Total Num. Parameters: 784 * 25 * 25 + 25 * 25 * 10 = 496,250</h5>
<p><img src="images/TensorNetwork/tensornet4a.png" width=350px></p>
<p>In this case I labeled each string with its dimension size, which is often called the <strong>bond dimension</strong>. As you'll see below, the 4 interior strings all have a bond dimension of 25.</p>
<div class="highlight"><pre><span></span><code><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">tensorNet2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sa,abc-&gt;sbc&#39;</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">A</span><span class="p">)))</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sbc,bct-&gt;st&#39;</span><span class="p">,(</span><span class="n">C</span><span class="p">,</span><span class="n">B</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</code></pre></div>

<h4>Train</h4>
<div class="highlight"><pre><span></span><code><span class="o">%%</span><span class="n">time</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">tensorNet2</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="o">/</span><span class="nv">Users</span><span class="o">/</span><span class="nv">brandonbrown</span><span class="o">/</span><span class="nv">anaconda3</span><span class="o">/</span><span class="nv">envs</span><span class="o">/</span><span class="nv">deeprl</span><span class="o">/</span><span class="nv">lib</span><span class="o">/</span><span class="nv">python3</span>.<span class="mi">6</span><span class="o">/</span><span class="nv">site</span><span class="o">-</span><span class="nv">packages</span><span class="o">/</span><span class="nv">ipykernel</span><span class="o">/</span><span class="nv">__main__</span>.<span class="nv">py</span>:<span class="mi">7</span>: <span class="nv">UserWarning</span>: <span class="nv">Implicit</span> <span class="nv">dimension</span> <span class="nv">choice</span> <span class="k">for</span> <span class="nv">softmax</span> <span class="nv">has</span> <span class="nv">been</span> <span class="nv">deprecated</span>. <span class="nv">Change</span> <span class="nv">the</span> <span class="k">call</span> <span class="nl">to</span> <span class="k">include</span> <span class="nv">dim</span><span class="o">=</span><span class="nv">X</span> <span class="nv">as</span> <span class="nv">an</span> <span class="nv">argument</span>.
<span class="o">/</span><span class="nv">Users</span><span class="o">/</span><span class="nv">brandonbrown</span><span class="o">/</span><span class="nv">anaconda3</span><span class="o">/</span><span class="nv">envs</span><span class="o">/</span><span class="nv">deeprl</span><span class="o">/</span><span class="nv">lib</span><span class="o">/</span><span class="nv">python3</span>.<span class="mi">6</span><span class="o">/</span><span class="nv">site</span><span class="o">-</span><span class="nv">packages</span><span class="o">/</span><span class="nv">ipykernel</span><span class="o">/</span><span class="nv">__main__</span>.<span class="nv">py</span>:<span class="mi">20</span>: <span class="nv">UserWarning</span>: <span class="nv">invalid</span> <span class="nv">index</span> <span class="nv">of</span> <span class="nv">a</span> <span class="mi">0</span><span class="o">-</span><span class="nv">dim</span> <span class="nv">tensor</span>. <span class="nv">This</span> <span class="nv">will</span> <span class="nv">be</span> <span class="nv">an</span> <span class="nv">error</span> <span class="nv">in</span> <span class="nv">PyTorch</span> <span class="mi">0</span>.<span class="mi">5</span>. <span class="nv">Use</span> <span class="nv">tensor</span>.<span class="nv">item</span><span class="ss">()</span> <span class="nv">to</span> <span class="nv">convert</span> <span class="nv">a</span> <span class="mi">0</span><span class="o">-</span><span class="nv">dim</span> <span class="nv">tensor</span> <span class="nv">to</span> <span class="nv">a</span> <span class="nv">Python</span> <span class="nv">number</span>
<span class="o">/</span><span class="nv">Users</span><span class="o">/</span><span class="nv">brandonbrown</span><span class="o">/</span><span class="nv">anaconda3</span><span class="o">/</span><span class="nv">envs</span><span class="o">/</span><span class="nv">deeprl</span><span class="o">/</span><span class="nv">lib</span><span class="o">/</span><span class="nv">python3</span>.<span class="mi">6</span><span class="o">/</span><span class="nv">site</span><span class="o">-</span><span class="nv">packages</span><span class="o">/</span><span class="nv">ipykernel</span><span class="o">/</span><span class="nv">__main__</span>.<span class="nv">py</span>:<span class="mi">45</span>: <span class="nv">UserWarning</span>: <span class="nv">invalid</span> <span class="nv">index</span> <span class="nv">of</span> <span class="nv">a</span> <span class="mi">0</span><span class="o">-</span><span class="nv">dim</span> <span class="nv">tensor</span>. <span class="nv">This</span> <span class="nv">will</span> <span class="nv">be</span> <span class="nv">an</span> <span class="nv">error</span> <span class="nv">in</span> <span class="nv">PyTorch</span> <span class="mi">0</span>.<span class="mi">5</span>. <span class="nv">Use</span> <span class="nv">tensor</span>.<span class="nv">item</span><span class="ss">()</span> <span class="nv">to</span> <span class="nv">convert</span> <span class="nv">a</span> <span class="mi">0</span><span class="o">-</span><span class="nv">dim</span> <span class="nv">tensor</span> <span class="nv">to</span> <span class="nv">a</span> <span class="nv">Python</span> <span class="nv">number</span>


<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">0</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">204465</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">795</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">1</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">199457</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">812</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">2</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">196803</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">823</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">3</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">194834</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">827</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">4</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">193722</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">837</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">5</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">191949</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">839</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">6</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">191317</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">844</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">7</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">190564</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">847</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">8</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">190050</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">850</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">9</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">189429</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">853</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">10</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">188894</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">856</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">11</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">188893</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">857</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">12</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">188916</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">859</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">13</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">187949</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">859</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">14</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">187448</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">861</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">15</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">187306</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">864</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">16</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">187102</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">863</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">17</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">187094</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">864</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">18</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">186368</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">866</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">19</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">186422</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">865</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">20</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">186429</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">867</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">21</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">185773</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">867</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">22</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">186021</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">867</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">23</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">185484</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">868</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">24</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">186032</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">870</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">25</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">185131</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">871</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">26</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">185170</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">872</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">27</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">185003</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">869</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">28</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184911</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">873</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">29</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184867</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">874</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">30</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184924</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">874</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">31</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184951</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">874</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">32</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184638</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">875</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">33</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184458</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">874</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">34</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184469</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">874</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">35</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184160</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">875</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">36</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183932</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">876</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">37</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184098</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">875</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">38</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184113</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">878</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">39</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183863</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">878</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">40</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183771</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">878</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">41</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">184383</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">877</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">42</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183535</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">878</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">43</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183703</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">878</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">44</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183593</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">878</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">45</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183371</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">878</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">46</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183289</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">880</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">47</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183360</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">879</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">48</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183175</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">881</span>
<span class="o">==&gt;&gt;&gt;</span> <span class="nv">epoch</span>: <span class="mi">49</span>, <span class="nv">batch</span> <span class="nv">index</span>: <span class="mi">100</span>, <span class="nv">test</span> <span class="nv">loss</span>: <span class="mi">2</span>.<span class="mi">183238</span>, <span class="nv">acc</span>: <span class="mi">0</span>.<span class="mi">880</span>
<span class="nv">CPU</span> <span class="nv">times</span>: <span class="nv">user</span> <span class="mi">40</span><span class="nv">min</span> <span class="mi">35</span><span class="nv">s</span>, <span class="nv">sys</span>: <span class="mi">2</span><span class="nv">min</span> <span class="mi">23</span><span class="nv">s</span>, <span class="nv">total</span>: <span class="mi">42</span><span class="nv">min</span> <span class="mi">58</span><span class="nv">s</span>
<span class="nv">Wall</span> <span class="nv">time</span>: <span class="mi">16</span><span class="nv">min</span> <span class="mi">17</span><span class="nv">s</span>
</code></pre></div>

<p>Not too bad right?</p>
<h3>Simple Non-Linear Tensor Network</h3>
<h4>Total Params: 784*20^2+4*20^2+20^4+20^3 = 479,200</h4>
<p><img src="images/TensorNetwork/tensornet4.png" width=500px></p>
<div class="highlight"><pre><span></span><code><span class="n">nl1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># a,b,c</span>
<span class="n">nl2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># c,e</span>
<span class="n">nl3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># d,f</span>
<span class="n">nl4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># e,f,g</span>
<span class="n">nl5</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># c,e</span>
<span class="n">nl6</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># d,f</span>
<span class="n">nl7</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># e,f,g</span>
<span class="k">def</span> <span class="nf">tensorNet3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sb,bcd-&gt;scd&#39;</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">nl1</span><span class="p">)))</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;scd,ce-&gt;se&#39;</span><span class="p">,(</span><span class="n">r1</span><span class="p">,</span><span class="n">nl2</span><span class="p">)))</span>
    <span class="n">r3</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;scd,df-&gt;sf&#39;</span><span class="p">,(</span><span class="n">r1</span><span class="p">,</span><span class="n">nl3</span><span class="p">)))</span>
    <span class="n">r4</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;se,sf,efgh-&gt;sgh&#39;</span><span class="p">,(</span><span class="n">r2</span><span class="p">,</span><span class="n">r3</span><span class="p">,</span><span class="n">nl4</span><span class="p">)))</span>
    <span class="n">r5</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;scd,ce-&gt;se&#39;</span><span class="p">,(</span><span class="n">r4</span><span class="p">,</span><span class="n">nl5</span><span class="p">)))</span>
    <span class="n">r6</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;scd,df-&gt;sf&#39;</span><span class="p">,(</span><span class="n">r4</span><span class="p">,</span><span class="n">nl6</span><span class="p">)))</span>
    <span class="n">r7</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;se,sf,efg-&gt;sg&#39;</span><span class="p">,(</span><span class="n">r5</span><span class="p">,</span><span class="n">r6</span><span class="p">,</span><span class="n">nl7</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">r7</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="o">%%</span><span class="n">time</span>
<span class="n">loss1b</span><span class="p">,</span> <span class="n">acc1b</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model1b</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">nl1</span><span class="p">,</span> <span class="n">nl2</span><span class="p">,</span> <span class="n">nl3</span><span class="p">,</span> <span class="n">nl4</span><span class="p">,</span> <span class="n">nl5</span><span class="p">,</span> <span class="n">nl6</span><span class="p">,</span> <span class="n">nl7</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,))</span>
</code></pre></div>

<p>Unfortunately, this non-linear tensor network performs significantly better with the training data (getting over 94% accuracy at 50 epochs) but the test accuracy tops out at around 88%, similar to the linear network. This just demonstrates that with the added non-linear ability, we can much more easily overfit the data. One thing to try is to add trainable bias tensors into the network as we have in neural networks.</p>
<h3>Conclusion</h3>
<p>Tensor Networks can be seen as a generalization of neural networks where the non-linearity arises purely out of the network topology as a manifestation of copying data, and not by a particular choice of non-linear activation function. They can be used just about anywhere a neural network can be (albeit with unproven performance) and tensor decompositions offer a powerful way to learn features from data. Tensor Networks are being actively studied and improved and I believe it is likely they will be a great tool in your machine learning toolkit.</p>
<h3>References</h3>
<ul>
<li>Yau, D. (2015). Operads of Wiring Diagrams. Retrieved from http://arxiv.org/abs/1512.01602</li>
<li>Maina, S. A. (2017). Graphical Linear Algebra.</li>
<li>Cichocki, A. (2014). Era of Big Data Processing: A New Approach via Tensor Networks and Tensor Decompositions, 1–30. http://doi.org/abs/1403.2048</li>
<li>Genovese, F. (2017). The Way of the Infinitesimal. Retrieved from http://arxiv.org/abs/1707.00459</li>
<li>Mangan, T. (2008). A gentle introduction to tensors. J. Biomedical Science and Engineering, 1, 64–67. http://doi.org/10.1016/S0004-3702(98)00053-8</li>
<li>Kissinger, A., &amp; Quick, D. (2015). A first-order logic for string diagrams, 171–189. http://doi.org/10.4230/LIPIcs.CALCO.2015.171</li>
<li>Barber, A. G. (1997). Linear Type Theories , Semantics and Action Calculi. Computing. Retrieved from http://hdl.handle.net/1842/392</li>
<li>Girard, J.-Y. (1995). Linear Logic: its syntax and semantics. Advances in Linear Logic, 1–42. http://doi.org/10.1017/CBO9780511629150.002</li>
<li>Wadler, P., Val, A., &amp; Arr, V. A. (1990). Philip Wadler University of Glasgow, (April), 1–21.</li>
<li>Martin, C. (2004). Tensor Decompositions Workshop Discussion Notes. Palo Alto, CA: American Institute of Mathematics (AIM), 1–27. Retrieved from http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Tensor+Decompositions+Workshop+Discussion+Notes#0</li>
<li>Kasai, H. (2017). Fast online low-rank tensor subspace tracking by CP decomposition using recursive least squares from incomplete observations, 1–21. Retrieved from http://arxiv.org/abs/1709.10276</li>
<li>Zhang, Y., Zhou, G., Zhao, Q., Cichocki, A., &amp; Wang, X. (2016). Fast nonnegative tensor factorization based on accelerated proximal gradient and low-rank approximation. Neurocomputing, 198, 148–154. http://doi.org/10.1016/j.neucom.2015.08.122</li>
<li>Smith, S., Beri, A., &amp; Karypis, G. (2017). Constrained Tensor Factorization with Accelerated AO-ADMM. Proceedings of the International Conference on Parallel Processing, 111–120. http://doi.org/10.1109/ICPP.2017.20</li>
<li>Stoudenmire, E. M., &amp; Schwab, D. J. (2016). Supervised Learning with Tensor Networks. Advances in Neural Information Processing Systems 29 (NIPS 2016), (Nips), 4799–4807. Retrieved from https://papers.nips.cc/paper/6211-supervised-learning-with-tensor-networks.pdf%0Ahttps://papers.nips.cc/paper/6211-supervised-learning-with-tensor-networksAdvances in Neural Information Processing Systems 29 (NIPS 2016)%0Ahttp://arxiv.org/abs/1605.05775</li>
<li>Rabanser, S., Shchur, O., &amp; Günnemann, S. (2017). Introduction to Tensor Decompositions and their Applications in Machine Learning, 1–13. Retrieved from http://arxiv.org/abs/1711.10781</li>
<li>Zafeiriou, S. (2009). Discriminant nonnegative tensor factorization algorithms. IEEE Transactions on Neural Networks, 20(2), 217–235. http://doi.org/10.1109/TNN.2008.2005293</li>
<li>Cohen, J. E., &amp; Gillis, N. (2018). Dictionary-Based Tensor Canonical Polyadic Decomposition. IEEE Transactions on Signal Processing, 66(7), 1876–1889. http://doi.org/10.1109/TSP.2017.2777393</li>
<li>Phan, A. H., &amp; Cichocki, A. (2010). Tensor decompositions for feature extraction and classification of high dimensional datasets. Nonlinear Theory and Its Applications, IEICE, 1(1), 37–68. http://doi.org/10.1587/nolta.1.37</li>
<li>Kossaifi, J., Lipton, Z. C., Khanna, A., Furlanello, T., &amp; Anandkumar, A. (2017). Tensor Contraction &amp; Regression Networks, 1–10. Retrieved from http://arxiv.org/abs/1707.08308</li>
<li>Ecognition, R., Lipton, Z. C., &amp; Anandkumar, A. (2018). D Eep a Ctive L Earning, 1–15.</li>
<li>Stoudenmire, E. M. (2017). Learning Relevant Features of Data with Multi-scale Tensor Networks, 1–12. http://doi.org/10.1088/2058-9565/aaba1a</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <div class="clear"></div>

                <div class="info">
                    <a href="http://outlace.com/TensorNets1.html">posted at 23:40</a>
                    by Brandon Brown
                    &nbsp;&middot;&nbsp;<a href="http://outlace.com/category/tensor-networks/" rel="tag">Tensor Networks</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://outlace.com/tag/tensor-networks/" class="tags">Tensor-Networks</a>
                </div>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'outlace';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
            </article>
            <div class="clear"></div>
            <footer>
                <p>
                <!--- <a href="http://outlace.com/feeds/all.atom.xml" rel="alternate">Atom Feed</a> --->
                <a href="mailto:outlacedev@gmail.com"><i class="svg-icon email"></i></a>
                <a href="http://github.com/outlace"><i class="svg-icon github"></i></a>
                <a href="http://outlace.com/feeds/all.atom.xml"><i class="svg-icon rss"></i></a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-65814776-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
</body>
</html>